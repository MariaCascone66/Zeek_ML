{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af56ba68",
   "metadata": {},
   "source": [
    "Preparazione pacchetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2beb06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa627031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Caricamento PARQUET: part-00000-1da06990-329c-4e38-913a-0f0aa39b388d-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-23fdcfa3-9dd3-4c72-886c-e945bfcf92e1-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-26e9208e-7819-451b-b23f-2e47f6d1e834-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-2b76f9cc-0710-45e4-9e33-98ad5808ee79-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-36240b61-b84f-4164-a873-d7973e652780-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-3f86626a-1225-47f9-a5a2-0170b737e404-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-745e350a-da9e-4619-bd52-8cc23bb41ad5-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-7c2e9adb-5430-4792-a42b-10ff5bbd46e8-c000.snappy.parquet\n",
      "[INFO] Caricamento CSV: part-00000-8c53ceaf-1fd1-4711-aa7d-26d0c5323dab-c000.csv\n",
      "[INFO] Caricamento PARQUET: part-00000-94d13437-ae00-4a8c-9f38-edd0196cfdee-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-9a46dd05-4b06-4a39-a45b-5c8460b6c37b-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-9ac876be-c07d-4a18-878d-959efa26f484-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-9aeb279c-81c6-4481-9b30-d35d4d194fea-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-b1a9fc13-8068-4a5d-91b2-871438709e81-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-b2b625bc-5816-4586-b977-35f9ed4487fd-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-be6d0798-554d-4c7a-9fef-d4c07aa0ce19-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-cbf26680-106d-40e7-8278-60520afdbb0e-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-d28b031b-bff1-4e16-853a-9b7d896627e7-c000.snappy.parquet\n",
      "[INFO] Caricamento CSV: part-00000-d32a9d5e-45b7-4e51-807e-1af297aba2df-c000.csv\n",
      "[INFO] Caricamento PARQUET: part-00000-d512890f-d1e9-49d5-a136-f87f0183cb4d-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-df678a79-4a73-452b-8e72-d624b2732f17-c000.snappy.parquet\n",
      "[INFO] Caricamento CSV: part-00000-e2ca97b6-6e3e-42bc-b674-04a5e4ffa9b3-c000.csv\n",
      "[INFO] Caricamento PARQUET: part-00000-ea53b0e8-d346-44e3-9a87-1f60ac35c610-c000.snappy.parquet\n",
      "[INFO] Caricamento PARQUET: part-00000-f9afaec0-242e-41e7-906d-a42681515d75-c000.snappy.parquet\n",
      "[INFO] Caricamento CSV: part-00000-fa66dc88-0b67-476a-81c7-7d788ad3873b-c000.csv\n",
      "[INFO] Dataset totale: 19262879 righe, 42 colonne\n",
      "[INFO] Colonne disponibili: ['resp_pkts', 'service', 'orig_ip_bytes', 'local_resp', 'missed_bytes', 'proto', 'duration', 'conn_state', 'dest_ip_zeek', 'orig_pkts', 'community_id', 'resp_ip_bytes', 'dest_port_zeek', 'orig_bytes', 'local_orig', 'datetime', 'history', 'resp_bytes', 'uid', 'src_port_zeek', 'ts', 'src_ip_zeek', 'label_tactic', 'label_technique', 'label_binary', '2021-12-12 - 2021-12-19', '2021-12-19 - 2021-12-26', '2021-12-26 - 2022-01-02', '2022-01-02 - 2022-01-09', '2022-08-28 - 2022-09-04', '2022-09-04 - 2022-09-11', '2022-09-11 - 2022-09-18', '2022-09-18 - 2022-09-25', '2022-09-25 - 2022-10-02', '2022-10-02 - 2022-10-09', '2022-10-09 - 2022-10-16', '2022-10-16 - 2022-10-23', '2022-10-23 - 2022-10-30', '2022-01-09 - 2022-01-16', '2022-01-16 - 2022-01-23', '2022-02-06 - 2022-02-13', '2022-02-13 - 2022-02-20']\n"
     ]
    }
   ],
   "source": [
    "# === 1. Imposta la cartella dove hai scaricato i dati ===\n",
    "DATASET_DIR = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\Final-Dataset\"\n",
    "\n",
    "# === 2. Carica tutti i file CSV e Parquet ===\n",
    "all_dfs = []\n",
    "\n",
    "for filename in os.listdir(DATASET_DIR):\n",
    "    file_path = os.path.join(DATASET_DIR, filename)\n",
    "\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(f\"[INFO] Caricamento CSV: {filename}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    elif filename.endswith(\".parquet\"):\n",
    "        print(f\"[INFO] Caricamento PARQUET: {filename}\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "if not all_dfs:\n",
    "    raise FileNotFoundError(f\"Nessun file CSV o Parquet trovato in {DATASET_DIR}\")\n",
    "\n",
    "# === 3. Unisci tutto in un unico DataFrame ===\n",
    "data = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"[INFO] Dataset totale: {data.shape[0]} righe, {data.shape[1]} colonne\")\n",
    "print(\"[INFO] Colonne disponibili:\", data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa0d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Numero e percentuale di valori nulli per colonna:\n",
      "                         null_count  null_percentage\n",
      "2022-01-16 - 2022-01-23    19262873        99.999969\n",
      "2022-01-09 - 2022-01-16    19262873        99.999969\n",
      "2022-02-06 - 2022-02-13    19262872        99.999964\n",
      "2022-02-13 - 2022-02-20    19262865        99.999927\n",
      "2022-10-09 - 2022-10-16    19262859        99.999896\n",
      "2022-09-04 - 2022-09-11    19262859        99.999896\n",
      "2022-09-11 - 2022-09-18    19262859        99.999896\n",
      "2022-10-02 - 2022-10-09    19262856        99.999881\n",
      "2022-09-18 - 2022-09-25    19262856        99.999881\n",
      "2022-01-02 - 2022-01-09    19262855        99.999875\n",
      "2021-12-12 - 2021-12-19    19262855        99.999875\n",
      "2021-12-19 - 2021-12-26    19262855        99.999875\n",
      "2021-12-26 - 2022-01-02    19262855        99.999875\n",
      "2022-10-23 - 2022-10-30    19262855        99.999875\n",
      "2022-08-28 - 2022-09-04    19262854        99.999870\n",
      "2022-10-16 - 2022-10-23    19262848        99.999839\n",
      "2022-09-25 - 2022-10-02    19262843        99.999813\n",
      "label_binary               18562531        96.364261\n",
      "label_technique            18562510        96.364152\n",
      "service                     9454625        49.082097\n",
      "resp_bytes                   826597         4.291139\n",
      "duration                     826597         4.291139\n",
      "orig_bytes                   826597         4.291139\n",
      "history                       78170         0.405806\n",
      "resp_pkts                        71         0.000369\n",
      "community_id                     71         0.000369\n",
      "orig_pkts                        71         0.000369\n",
      "orig_ip_bytes                    71         0.000369\n",
      "local_resp                       71         0.000369\n",
      "missed_bytes                     71         0.000369\n",
      "proto                            71         0.000369\n",
      "conn_state                       71         0.000369\n",
      "dest_ip_zeek                     71         0.000369\n",
      "src_ip_zeek                      71         0.000369\n",
      "ts                               71         0.000369\n",
      "resp_ip_bytes                    71         0.000369\n",
      "dest_port_zeek                   71         0.000369\n",
      "local_orig                       71         0.000369\n",
      "datetime                         71         0.000369\n",
      "uid                              71         0.000369\n",
      "src_port_zeek                    71         0.000369\n",
      "label_tactic                     37         0.000192\n",
      "\n",
      "[INFO] Prime righe con etichette espanse:\n",
      "   tactic_Collection  tactic_Command and Control  tactic_Credential Access  tactic_Defense Evasion  tactic_Discovery  tactic_Execution  tactic_Exfiltration  tactic_Initial Access  \\\n",
      "0              False                       False                     False                    True             False             False                False                  False   \n",
      "1              False                       False                     False                   False             False             False                False                   True   \n",
      "2              False                       False                     False                   False             False             False                False                  False   \n",
      "3              False                       False                     False                   False             False             False                False                  False   \n",
      "4              False                       False                     False                   False             False             False                 True                  False   \n",
      "\n",
      "   tactic_Lateral Movement  tactic_Persistence  tactic_Privilege Escalation  tactic_Reconnaissance  tactic_Resource Development  tactic_dt_first_record  tactic_dt_last_record  tactic_dt_week_end  \\\n",
      "0                    False               False                        False                  False                        False                   False                  False               False   \n",
      "1                    False               False                        False                  False                        False                   False                  False               False   \n",
      "2                    False                True                        False                  False                        False                   False                  False               False   \n",
      "3                    False               False                         True                  False                        False                   False                  False               False   \n",
      "4                    False               False                        False                  False                        False                   False                  False               False   \n",
      "\n",
      "   tactic_dt_week_start  tactic_none  tactic_total_record_count  technique_Duplicate  technique_T1046  technique_T1059  technique_T1071  technique_T1112  technique_T1133  technique_T1136  \\\n",
      "0                 False        False                      False                False            False            False            False            False            False            False   \n",
      "1                 False        False                      False                False            False            False            False            False            False            False   \n",
      "2                 False        False                      False                False            False            False            False            False            False            False   \n",
      "3                 False        False                      False                False            False            False            False            False            False            False   \n",
      "4                 False        False                      False                False            False            False            False            False            False            False   \n",
      "\n",
      "   technique_T1190  technique_T1203  technique_T1204  technique_T1210  technique_T1505  technique_T1546  technique_T1547  technique_T1548  technique_T1557  technique_T1566  technique_T1571  \\\n",
      "0            False            False            False            False            False            False            False            False            False            False            False   \n",
      "1            False            False            False            False            False            False            False            False            False            False            False   \n",
      "2            False            False            False            False            False            False            False            False            False            False            False   \n",
      "3            False            False            False            False            False            False            False            False            False            False            False   \n",
      "4            False            False            False            False            False            False            False            False            False            False            False   \n",
      "\n",
      "   technique_T1587  technique_T1589  technique_T1590  technique_T1592  technique_T1595  technique_dt_first_record  technique_dt_last_record  technique_dt_week_end  technique_dt_week_start  \\\n",
      "0            False            False            False            False            False                      False                     False                  False                    False   \n",
      "1            False            False            False            False            False                      False                     False                  False                    False   \n",
      "2            False            False            False            False            False                      False                     False                  False                    False   \n",
      "3            False            False            False            False            False                      False                     False                  False                    False   \n",
      "4            False            False            False            False            False                      False                     False                  False                    False   \n",
      "\n",
      "   technique_none  technique_total_record_count  \n",
      "0           False                         False  \n",
      "1           False                         False  \n",
      "2           False                         False  \n",
      "3           False                         False  \n",
      "4           False                         False  \n",
      "\n",
      "[INFO] File Excel 'dataset_preview.xlsx' creato con i primi 30 elementi (etichette espanse incluse).\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Mostra tutte le colonne senza troncamenti\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# === 🔹 Conteggio dei valori nulli per colonna + percentuale ===\n",
    "null_counts = data.isna().sum()\n",
    "null_percent = (null_counts / len(data)) * 100\n",
    "null_table = pd.DataFrame({\n",
    "    'null_count': null_counts,\n",
    "    'null_percentage': null_percent\n",
    "}).sort_values(by='null_count', ascending=False)\n",
    "\n",
    "print(\"\\n[INFO] Numero e percentuale di valori nulli per colonna:\")\n",
    "print(null_table)\n",
    "\n",
    "# === 🔹 One-hot encoding di label_tactic e label_technique ===\n",
    "tactic_dummies = pd.get_dummies(data['label_tactic'], prefix='tactic')\n",
    "technique_dummies = pd.get_dummies(data['label_technique'], prefix='technique')\n",
    "\n",
    "# 🔹 Unisci le nuove colonne al DataFrame originale\n",
    "data_expanded = pd.concat([data, tactic_dummies, technique_dummies], axis=1)\n",
    "\n",
    "# Mostra un esempio solo con le etichette espanse\n",
    "expanded_cols = tactic_dummies.columns.tolist() + technique_dummies.columns.tolist()\n",
    "print(\"\\n[INFO] Prime righe con etichette espanse:\")\n",
    "print(data_expanded[expanded_cols].head(5))\n",
    "\n",
    "# === 🔹 Salva i primi 30 elementi in un file Excel ===\n",
    "data_expanded.head(30).to_excel(\"dataset_preview.xlsx\", index=False)\n",
    "print(\"\\n[INFO] File Excel 'dataset_preview.xlsx' creato con i primi 30 elementi (etichette espanse incluse).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8932ab",
   "metadata": {},
   "source": [
    "Setup, colonne numeriche e ricerca colonne da rimuovere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbde4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Colonne da rimuovere (>99% null): 0 -> []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_DIR = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\Final-Dataset\"\n",
    "\n",
    "# 🔹 Colonne numeriche da trattare\n",
    "numerical_cols = [\n",
    "    'orig_bytes', 'resp_bytes', 'duration', 'orig_pkts', \n",
    "    'resp_pkts', 'orig_ip_bytes', 'resp_ip_bytes', 'missed_bytes'\n",
    "]\n",
    "\n",
    "# 🔹 Funzione per identificare colonne con >99% valori nulli\n",
    "def get_high_null_columns(file_path, threshold=99.0, chunk_size=500_000):\n",
    "    null_counts_total = None\n",
    "    total_rows = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "        if null_counts_total is None:\n",
    "            null_counts_total = chunk.isna().sum()\n",
    "        else:\n",
    "            null_counts_total += chunk.isna().sum()\n",
    "    null_percent = (null_counts_total / total_rows) * 100\n",
    "    return null_percent[null_percent > threshold].index.tolist()\n",
    "\n",
    "cols_to_drop = []\n",
    "for filename in os.listdir(DATASET_DIR):\n",
    "    file_path = os.path.join(DATASET_DIR, filename)\n",
    "    if filename.endswith(\".csv\"):\n",
    "        cols_to_drop.extend(get_high_null_columns(file_path))\n",
    "\n",
    "cols_to_drop = list(set(cols_to_drop))\n",
    "print(f\"[INFO] Colonne da rimuovere (>99% null): {len(cols_to_drop)} -> {cols_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a821b",
   "metadata": {},
   "source": [
    "Riempimento NaN e codifica etichette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85900602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pulizia valori nulli completata\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 🔹 Calcolo media globale per colonne numeriche\n",
    "global_means = data_clean[numerical_cols].mean()\n",
    "data_clean[numerical_cols] = data_clean[numerical_cols].fillna(global_means)\n",
    "# Se colonna era completamente NaN -> riempi con 0\n",
    "data_clean[numerical_cols] = data_clean[numerical_cols].fillna(0)\n",
    "\n",
    "# 🔹 Riempimento NaN in colonne categoriche\n",
    "for col in data_clean.select_dtypes(include='category').columns:\n",
    "    data_clean[col] = data_clean[col].cat.add_categories([\"unknown\"]).fillna(\"unknown\")\n",
    "\n",
    "# 🔹 Codifica etichette\n",
    "label_encoder_tactic = LabelEncoder()\n",
    "data_clean['label_tactic_encoded'] = label_encoder_tactic.fit_transform(data_clean['label_tactic'])\n",
    "\n",
    "label_encoder_technique = LabelEncoder()\n",
    "data_clean['label_technique_encoded'] = label_encoder_technique.fit_transform(data_clean['label_technique'])\n",
    "\n",
    "print(\"[INFO] Pulizia valori nulli completata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c030df",
   "metadata": {},
   "source": [
    "Filtraggio tattiche valide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d43eb2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset dopo filtraggio: 19262781 righe\n",
      "[INFO] Distribuzione classi label_tactic:\n",
      "label_tactic\n",
      "none                    9631940\n",
      "Reconnaissance          9330216\n",
      "Resource Development     275476\n",
      "Discovery                 18907\n",
      "Privilege Escalation       3081\n",
      "Defense Evasion            3067\n",
      "Credential Access            34\n",
      "Initial Access               22\n",
      "Lateral Movement             17\n",
      "Persistence                  13\n",
      "Exfiltration                  8\n",
      "Name: count, dtype: int64\n",
      "['Defense Evasion' 'Initial Access' 'Persistence' 'Privilege Escalation'\n",
      " 'Exfiltration' 'Lateral Movement' 'Reconnaissance' 'Credential Access'\n",
      " 'Resource Development' 'none' 'Discovery']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "valid_tactics = [\n",
    "    \"Reconnaissance\", \"Discovery\", \"Credential Access\", \"Privilege Escalation\",\n",
    "    \"Exfiltration\", \"Lateral Movement\", \"Resource Development\", \"Initial Access\",\n",
    "    \"Persistence\", \"Defense Evasion\"\n",
    "]\n",
    "\n",
    "# 🔹 Filtra solo tattiche valide + none\n",
    "data_clean = data_clean[data_clean['label_tactic'].isin(valid_tactics + [\"none\"])]\n",
    "\n",
    "# 🔹 Rimuovi eventuali etichette spurie\n",
    "data_clean = data_clean[~data_clean['label_tactic'].isin([\n",
    "    \"unknown\", \"dt_first_record\", \"dt_last_record\", \n",
    "    \"dt_week_end\", \"dt_week_start\", \"total_record_count\"\n",
    "])]\n",
    "\n",
    "# 🔹 Ricodifica le etichette dopo il filtraggio\n",
    "data_clean['label_tactic_encoded'] = LabelEncoder().fit_transform(data_clean['label_tactic'])\n",
    "\n",
    "print(f\"[INFO] Dataset dopo filtraggio: {data_clean.shape[0]} righe\")\n",
    "print(\"[INFO] Distribuzione classi label_tactic:\")\n",
    "print(data_clean['label_tactic'].value_counts())\n",
    "\n",
    "print(data_clean['label_tactic'].unique())\n",
    "print(data_clean['label_tactic'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894333f",
   "metadata": {},
   "source": [
    "Standardizzazione + PCA incrementale (2 passate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "890937d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] StandardScaler fitted su tutto il dataset\n",
      "[INFO] PCA incrementale fitted su tutto il dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "\n",
    "chunksize = 100_000\n",
    "\n",
    "# 🔹 Prima passata: calcola media e std globali con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for start in range(0, len(data_clean), chunksize):\n",
    "    batch = data_clean[numerical_cols].iloc[start:start+chunksize].values.astype(np.float32)\n",
    "    scaler.partial_fit(batch)\n",
    "print(\"[INFO] StandardScaler fitted su tutto il dataset\")\n",
    "\n",
    "# 🔹 Seconda passata: PCA incrementale sui dati normalizzati\n",
    "pca = IncrementalPCA(n_components=2)\n",
    "for start in range(0, len(data_clean), chunksize):\n",
    "    batch = data_clean[numerical_cols].iloc[start:start+chunksize].values.astype(np.float32)\n",
    "    batch_scaled = scaler.transform(batch)\n",
    "    pca.partial_fit(batch_scaled)\n",
    "print(\"[INFO] PCA incrementale fitted su tutto il dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae259bb",
   "metadata": {},
   "source": [
    "Trasformazione finale + salvataggio su Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c60681c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PCA batch salvata in cartella: dataset_pca_incremental\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "output_dir = \"dataset_pca_incremental\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for start in range(0, len(data_clean), chunksize):\n",
    "    batch = data_clean[numerical_cols].iloc[start:start+chunksize].values.astype(np.float32)\n",
    "    batch_scaled = scaler.transform(batch)\n",
    "    batch_pca = pca.transform(batch_scaled)\n",
    "    \n",
    "    batch_labels = data_clean['label_tactic'].iloc[start:start+chunksize].values\n",
    "    table = pa.Table.from_arrays(\n",
    "        [pa.array(batch_pca[:,0], type=pa.float32()),\n",
    "         pa.array(batch_pca[:,1], type=pa.float32()),\n",
    "         pa.array(batch_labels.astype(str))],\n",
    "        names=['PC1','PC2','label_tactic']\n",
    "    )\n",
    "    pq.write_to_dataset(table, root_path=output_dir)\n",
    "\n",
    "print(f\"[INFO] PCA batch salvata in cartella: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90b59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ricaricamento dataset PCA da: dataset_pca_incremental\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m pca_batches = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Ricaricamento dataset PCA da: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpca_dataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCaricamento batch PCA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     15\u001b[39m     table = pq.read_table(file)\n\u001b[32m     16\u001b[39m     df = table.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maria\\anaconda3\\envs\\zeek-ml\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maria\\anaconda3\\envs\\zeek-ml\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "\n",
    "# === 1️⃣ Trova tutti i file parquet nella cartella ===\n",
    "pca_dataset_path = \"dataset_pca_incremental\"\n",
    "parquet_files = glob.glob(f\"{pca_dataset_path}/**/*.parquet\", recursive=True)\n",
    "\n",
    "pca_batches = []\n",
    "print(f\"[INFO] Ricaricamento dataset PCA da: {pca_dataset_path}\")\n",
    "\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento batch PCA\"):\n",
    "    table = pq.read_table(file)\n",
    "    df = table.to_pandas()\n",
    "    pca_batches.append(df)\n",
    "\n",
    "pca_df = pd.concat(pca_batches, ignore_index=True)\n",
    "print(f\"[INFO] Dataset PCA caricato: {pca_df.shape[0]} righe, {pca_df.shape[1]} colonne\")\n",
    "\n",
    "# === 2️⃣ Controllo delle classi ===\n",
    "print(\"\\n[INFO] Distribuzione classi dopo PCA:\")\n",
    "print(pca_df['label_tactic'].value_counts())\n",
    "\n",
    "# === 3️⃣ Scatter plot (usando un campione per non saturare la RAM) ===\n",
    "sample_df = pca_df.sample(min(200_000, len(pca_df)))  # massimo 200k punti per leggibilità\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    sample_df['PC1'],\n",
    "    sample_df['PC2'],\n",
    "    c=sample_df['label_tactic'].astype('category').cat.codes,\n",
    "    s=1,\n",
    "    cmap='tab10',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(\"Distribuzione PCA (PC1 vs PC2)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 4️⃣ Salva un campione casuale per analisi offline ===\n",
    "sample_df.to_csv(\"pca_sample.csv\", index=False)\n",
    "print(\"[INFO] Campione PCA salvato in 'pca_sample.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa06776",
   "metadata": {},
   "source": [
    "Splitting di dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# (Opzionale: salva i dati pre-processati per usarli in un altro script)\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)\n",
    "\n",
    "print(\"[INFO] Preprocessing completato ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
