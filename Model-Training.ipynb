{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620560fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 1: Lettura dataset salvati\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "output_folder = \"model_data\"\n",
    "\n",
    "# Carica dataset se non lo hai gi√†\n",
    "X_train = pd.read_parquet(os.path.join(output_folder, \"X_autoencoder.parquet\"))\n",
    "y_train = pd.read_csv(os.path.join(output_folder, \"y_multiclass.csv\")).squeeze()\n",
    "\n",
    "# Cartella dei dati salvati\n",
    "data_folder = \"model_data\"\n",
    "\n",
    "# 1Ô∏è‚É£ Lettura train/test set\n",
    "X_train = pd.read_csv(os.path.join(data_folder, \"X_train_balanced.csv\"))\n",
    "X_test  = pd.read_csv(os.path.join(data_folder, \"X_test_balanced.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(data_folder, \"y_train_balanced.csv\")).squeeze()\n",
    "y_test  = pd.read_csv(os.path.join(data_folder, \"y_test_balanced.csv\")).squeeze()\n",
    "\n",
    "# 2Ô∏è‚É£ Lettura scaler (opzionale se serve per nuovi dati)\n",
    "scaler_latent = joblib.load(os.path.join(data_folder, \"scaler_latent.pkl\"))\n",
    "\n",
    "# 3Ô∏è‚É£ Controllo dimensioni e classi\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"y_test distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2: Random Forest con CV, metriche train vs test, learning curve e valutazione overfitting\n",
    "# ==========================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, precision_recall_curve, roc_curve, auc, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"üèóÔ∏è Addestramento Random Forest con 5-fold CV e GridSearch (parametri moderati)...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Modello base\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ GridSearchCV con parametri conservativi per evitare overfitting\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Fit su train set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 4Ô∏è‚É£ Miglior modello\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(f\"\\nüèÜ Miglior combinazione iperparametri: {grid_search.best_params_}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Predizioni su train e test set\n",
    "y_train_pred = best_rf.predict(X_train)\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# 6Ô∏è‚É£ Metriche sul train set\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Metriche sul TRAIN set:\")\n",
    "print(f\"Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall:    {train_recall:.4f}\")\n",
    "print(f\"F1-score:  {train_f1:.4f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Metriche sul test set\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "specificity = cm.diagonal() / (cm.sum(axis=1) - cm.diagonal() + cm.diagonal())\n",
    "\n",
    "print(\"\\nüìä Metriche sul TEST set:\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-score:  {test_f1:.4f}\")\n",
    "for i, cls in enumerate(best_rf.classes_):\n",
    "    print(f\"Specificity classe '{cls}': {specificity[i]:.4f}\")\n",
    "\n",
    "# 8Ô∏è‚É£ Valutazione automatica overfitting / underfitting\n",
    "gap_f1 = train_f1 - test_f1\n",
    "if gap_f1 > 0.05:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile OVERFITTING: gap F1 train-test = {gap_f1:.4f}\")\n",
    "elif test_f1 < 0.7:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile UNDERFITTING: F1 test = {test_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Modello bilanciato, nessun evidente overfitting/underfitting\")\n",
    "\n",
    "# ==========================================================\n",
    "# Learning Curve train vs test\n",
    "# ==========================================================\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_rf, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Train score\", marker='o')\n",
    "plt.plot(train_sizes, test_mean, label=\"Test score\", marker='s')\n",
    "plt.xlabel(\"Numero di campioni di training\")\n",
    "plt.ylabel(\"F1-score macro\")\n",
    "plt.title(\"üìà Learning Curve Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° Interpretazione Learning Curve:\n",
    "- Gap elevato tra train e test ‚Üí possibile OVERFITTING.\n",
    "- Train e test vicini e alti ‚Üí modello generalizza bene.\n",
    "- Entrambi bassi ‚Üí possibile UNDERFITTING.\n",
    "\"\"\")\n",
    "\n",
    "# ==========================================================\n",
    "# Grafici e metriche visuali\n",
    "# ==========================================================\n",
    "print(\"üìä Generazione grafici e visualizzazioni...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Confusion Matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap='Blues', normalize='true')\n",
    "plt.title(\"üìä Confusion Matrix Normalizzata\")\n",
    "plt.show()\n",
    "print(\"üí° Interpretazione: valori vicini a 1 sulla diagonale indicano buona capacit√† predittiva per ogni classe.\")\n",
    "\n",
    "# 2Ô∏è‚É£ Precision-Recall Curve Multiclass\n",
    "y_test_bin = label_binarize(y_test, classes=best_rf.classes_)\n",
    "y_score = best_rf.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(best_rf.classes_):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f\"{cls} (AP={ap:.2f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"üìà Precision-Recall Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"üí° Interpretazione: curve vicine all'angolo in alto a destra ‚Üí buone performance; gap tra classi ‚Üí sbilanciamento o difficolt√† su alcune classi.\")\n",
    "\n",
    "# 3Ô∏è‚É£ ROC Curve Multiclass\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(best_rf.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{cls} (AUC={roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"üìà ROC Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"üí° Interpretazione: AUC vicino a 1 indica ottima capacit√† discriminativa; valori bassi ‚Üí difficolt√† nel distinguere classi.\")\n",
    "\n",
    "print(\"‚úÖ Tutti i grafici e metriche generati con successo.\")\n",
    "\n",
    "# Feature Importance RF\n",
    "importances = best_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # ordina decrescente\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances[indices],\n",
    "    y=X_train.columns[indices],\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"üåü Feature Importance Random Forest\")\n",
    "plt.xlabel(\"Importanza\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b69c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO MLP: Metriche train vs test, learning curve e visualizzazioni\n",
    "# ==========================================================\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, precision_recall_curve, roc_curve, auc, average_precision_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"üèóÔ∏è Addestramento MLPClassifier con 5-fold CV e parametri standard...\")\n",
    "\n",
    "# 1Ô∏è‚É£ MLP base\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,\n",
    "    batch_size=32,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Cross-validation stratificata (opzionale GridSearch per iperparametri)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 3Ô∏è‚É£ Fit su train set\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 4Ô∏è‚É£ Predizioni su train e test set\n",
    "y_train_pred = mlp.predict(X_train)\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# 5Ô∏è‚É£ Metriche sul train set\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Metriche sul TRAIN set:\")\n",
    "print(f\"Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall:    {train_recall:.4f}\")\n",
    "print(f\"F1-score:  {train_f1:.4f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Metriche sul test set\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "specificity = cm.diagonal() / (cm.sum(axis=1) - cm.diagonal() + cm.diagonal())\n",
    "\n",
    "print(\"\\nüìä Metriche sul TEST set:\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-score:  {test_f1:.4f}\")\n",
    "for i, cls in enumerate(mlp.classes_):\n",
    "    print(f\"Specificity classe '{cls}': {specificity[i]:.4f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Valutazione overfitting / underfitting\n",
    "gap_f1 = train_f1 - test_f1\n",
    "if gap_f1 > 0.05:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile OVERFITTING: gap F1 train-test = {gap_f1:.4f}\")\n",
    "elif test_f1 < 0.7:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile UNDERFITTING: F1 test = {test_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Modello bilanciato, nessun evidente overfitting/underfitting\")\n",
    "\n",
    "# ==========================================================\n",
    "# Learning Curve train vs test\n",
    "# ==========================================================\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    mlp, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Train score\", marker='o')\n",
    "plt.plot(train_sizes, test_mean, label=\"Test score\", marker='s')\n",
    "plt.xlabel(\"Numero di campioni di training\")\n",
    "plt.ylabel(\"F1-score macro\")\n",
    "plt.title(\"üìà Learning Curve MLP\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Grafici e metriche visuali\n",
    "# ==========================================================\n",
    "print(\"üìä Generazione grafici e visualizzazioni...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Confusion Matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test, cmap='Blues', normalize='true')\n",
    "plt.title(\"üìä Confusion Matrix Normalizzata\")\n",
    "plt.show()\n",
    "\n",
    "# 2Ô∏è‚É£ Precision-Recall Curve Multiclass\n",
    "y_test_bin = label_binarize(y_test, classes=mlp.classes_)\n",
    "y_score = mlp.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(mlp.classes_):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f\"{cls} (AP={ap:.2f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"üìà Precision-Recall Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3Ô∏è‚É£ ROC Curve Multiclass\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(mlp.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{cls} (AUC={roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"üìà ROC Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Permutation Feature Importance MLP\n",
    "# ==========================================================\n",
    "perm_importance = permutation_importance(\n",
    "    mlp, X_test, y_test, n_repeats=10, random_state=42, scoring='f1_macro'\n",
    ")\n",
    "sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=perm_importance.importances_mean[sorted_idx],\n",
    "    y=X_train.columns[sorted_idx],\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"üåü Permutation Feature Importance MLP\")\n",
    "plt.xlabel(\"Importanza media (F1 macro)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO LightGBM con CV, metriche train vs test, learning curve\n",
    "# ==========================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, precision_recall_curve, roc_curve, auc, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"üèóÔ∏è Addestramento LightGBM con 5-fold CV e GridSearch (parametri moderati)...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Modello base\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ GridSearchCV per iperparametri conservativi\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 63],\n",
    "    'max_depth': [10, 20],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Fit su train set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 4Ô∏è‚É£ Miglior modello\n",
    "best_lgb = grid_search.best_estimator_\n",
    "print(f\"\\nüèÜ Miglior combinazione iperparametri: {grid_search.best_params_}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Predizioni su train e test set\n",
    "y_train_pred = best_lgb.predict(X_train)\n",
    "y_test_pred = best_lgb.predict(X_test)\n",
    "\n",
    "# 6Ô∏è‚É£ Metriche sul train set\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "print(\"\\nüìä Metriche sul TRAIN set:\")\n",
    "print(f\"Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall:    {train_recall:.4f}\")\n",
    "print(f\"F1-score:  {train_f1:.4f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Metriche sul test set\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "specificity = cm.diagonal() / (cm.sum(axis=1) - cm.diagonal() + cm.diagonal())\n",
    "\n",
    "print(\"\\nüìä Metriche sul TEST set:\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-score:  {test_f1:.4f}\")\n",
    "for i, cls in enumerate(best_lgb.classes_):\n",
    "    print(f\"Specificity classe '{cls}': {specificity[i]:.4f}\")\n",
    "\n",
    "# 8Ô∏è‚É£ Valutazione over/underfitting\n",
    "gap_f1 = train_f1 - test_f1\n",
    "if gap_f1 > 0.05:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile OVERFITTING: gap F1 train-test = {gap_f1:.4f}\")\n",
    "elif test_f1 < 0.7:\n",
    "    print(f\"\\n‚ö†Ô∏è Possibile UNDERFITTING: F1 test = {test_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Modello bilanciato, nessun evidente overfitting/underfitting\")\n",
    "\n",
    "# ==========================================================\n",
    "# Learning Curve train vs test\n",
    "# ==========================================================\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_lgb, X_train, y_train, cv=5, scoring='f1_macro', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Train score\", marker='o')\n",
    "plt.plot(train_sizes, test_mean, label=\"Test score\", marker='s')\n",
    "plt.xlabel(\"Numero di campioni di training\")\n",
    "plt.ylabel(\"F1-score macro\")\n",
    "plt.title(\"üìà Learning Curve LightGBM\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Confusion Matrix\n",
    "# ==========================================================\n",
    "plt.figure(figsize=(8,6))\n",
    "ConfusionMatrixDisplay.from_estimator(best_lgb, X_test, y_test, cmap='Blues', normalize='true')\n",
    "plt.title(\"üìä Confusion Matrix Normalizzata\")\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Precision-Recall Curve Multiclass\n",
    "# ==========================================================\n",
    "y_test_bin = label_binarize(y_test, classes=best_lgb.classes_)\n",
    "y_score = best_lgb.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(best_lgb.classes_):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    ap = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    plt.plot(recall, precision, lw=2, label=f\"{cls} (AP={ap:.2f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"üìà Precision-Recall Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# ROC Curve Multiclass\n",
    "# ==========================================================\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cls in enumerate(best_lgb.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{cls} (AUC={roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"üìà ROC Curve Multiclass\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Feature Importance\n",
    "# ==========================================================\n",
    "importances = best_lgb.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances[indices], y=X_train.columns[indices], palette=\"viridis\")\n",
    "plt.title(\"üåü Feature Importance LightGBM\")\n",
    "plt.xlabel(\"Importanza\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29ce39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
