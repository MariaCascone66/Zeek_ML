{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b447033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO: Data Cleaning e Aggregazione ottimizzato (ZeekDataFall22)\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# -----------------------\n",
    "# 1Ô∏è‚É£ Caricamento incrementale\n",
    "# -----------------------\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe iniziali: {len(data)}\")\n",
    "print(f\"Totale colonne iniziali: {data.shape[1]}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2Ô∏è‚É£ Conversione object ‚Üí category per risparmio RAM\n",
    "# -----------------------\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# -----------------------\n",
    "# 3Ô∏è‚É£ Analisi valori mancanti\n",
    "# -----------------------\n",
    "col_summary = pd.DataFrame({\n",
    "    'dtype': data.dtypes,\n",
    "    'num_missing': data.isna().sum(),\n",
    "    'perc_missing': data.isna().mean() * 100\n",
    "}).sort_values('perc_missing', ascending=False)\n",
    "display(col_summary)\n",
    "\n",
    "# -----------------------\n",
    "# 4Ô∏è‚É£ Eliminazione colonne con troppi NaN (>50%)\n",
    "# -----------------------\n",
    "threshold = 50\n",
    "cols_to_drop = col_summary[col_summary['perc_missing'] > threshold].index.tolist()\n",
    "if cols_to_drop:\n",
    "    data.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Colonne eliminate per troppi NaN (> {threshold}%): {cols_to_drop}\")\n",
    "else:\n",
    "    print(\"Nessuna colonna da eliminare per troppi NaN.\")\n",
    "\n",
    "# -----------------------\n",
    "# 5Ô∏è‚É£ Rimozione duplicati (solo colonne chiave per risparmio RAM)\n",
    "# -----------------------\n",
    "subset_cols = ['uid', 'ts', 'orig_bytes', 'resp_bytes'] if 'uid' in data.columns else None\n",
    "duplicates_before = data.duplicated(subset=subset_cols).sum()\n",
    "data = data.drop_duplicates(subset=subset_cols)\n",
    "print(f\"Duplicati rimossi: {duplicates_before}\")\n",
    "\n",
    "# -----------------------\n",
    "# 6Ô∏è‚É£ Imputazione valori mancanti\n",
    "# -----------------------\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "for col in num_cols:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "for col in cat_cols:\n",
    "    if data[col].isna().any():\n",
    "        mode_val = data[col].mode()\n",
    "        data[col].fillna(mode_val[0] if not mode_val.empty else 'unknown', inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# 7Ô∏è‚É£ Aggregazioni session-level features\n",
    "# -----------------------\n",
    "if 'uid' not in data.columns:\n",
    "    data['uid'] = range(len(data))\n",
    "\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8Ô∏è‚É£ Anteprima finale\n",
    "# -----------------------\n",
    "display(data.head())\n",
    "print(\"‚úÖ Data Cleaning e Aggregazioni completati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2c: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Controllo colonna obbligatoria\n",
    "if 'label_technique' not in data.columns:\n",
    "    raise KeyError(\"‚ö†Ô∏è Manca la colonna 'label_technique' nel dataset caricato.\")\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"‚ö†Ô∏è Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Grafico prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione benigni -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"‚úÖ Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\nüìä Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento tactic -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\nüìä Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3a: Controllo valori nulli e riepilogo colonne/feature\n",
    "# ==========================================================\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"‚úÖ Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\nüìä Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\nüîπ Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\nüîπ Feature categoriali:\")\n",
    "print(cat_features)\n",
    "# --------------------------\n",
    "# Salvataggio feature categoriali\n",
    "# --------------------------\n",
    "cat_features_df = data[cat_features].copy()\n",
    "cat_features_df.to_parquet(r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\categorical_features.parquet\", index=False)\n",
    "print(f\"üíæ Feature categoriali salvate: {cat_features_df.shape[1]} colonne, {cat_features_df.shape[0]} righe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Selezione feature numeriche\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "selected_features = variance[variance > 0.01].index.tolist()\n",
    "print(f\"‚úÖ Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "for col in selected_features:\n",
    "    Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    outlier_summary[col] = ((data[col]<lower) | (data[col]>upper)).sum()\n",
    "print(\"‚ö†Ô∏è Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Trasformazione outlier (Winsorization + log)\n",
    "# ================================\n",
    "print(\"üèóÔ∏è Applicazione trasformazioni per gestire outlier...\\n\")\n",
    "for col in selected_features:\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# üîß Fix: rimozione eventuali NaN residui\n",
    "data = data.dropna(subset=selected_features)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Analisi post-transform\n",
    "# ================================\n",
    "print(\"\\nüìÑ REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "print(\"üîπ Statistiche descrittive:\")\n",
    "display(data[selected_features].describe().T)\n",
    "\n",
    "print(\"\\nüìù Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere le distribuzioni.\\n\"\n",
    "    \"- Winsorization: limita i valori ai percentili 1% e 99%, riducendo l'impatto degli outlier.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria e migliora la stabilit√† numerica.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni pi√π compatte e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b893b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"üìä Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"üìä Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"‚öñÔ∏è Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "joblib.dump(class_weights_dict, \"model_data/class_weights_dict.pkl\")\n",
    "print(\"‚úÖ Class weights salvati in 'model_data/class_weights_dict.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3f: Creazione dataset bilanciato (3 classi principali)\n",
    "# ==========================================================\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"‚öñÔ∏è Creazione dataset bilanciato tra le tre classi principali...\\n\")\n",
    "\n",
    "# Filtra solo le 3 classi principali\n",
    "balanced_data = data[data['label_tactic_reduced'].isin(['Resource Development', 'Reconnaissance', 'Discovery'])]\n",
    "\n",
    "# Trova la classe pi√π piccola\n",
    "min_count = balanced_data['label_tactic_reduced'].value_counts().min()\n",
    "\n",
    "# Esegui sottocampionamento per bilanciare\n",
    "balanced_samples = []\n",
    "for cls in ['Resource Development', 'Reconnaissance', 'Discovery']:\n",
    "    cls_df = balanced_data[balanced_data['label_tactic_reduced'] == cls]\n",
    "    cls_down = resample(cls_df, replace=False, n_samples=min_count, random_state=42)\n",
    "    balanced_samples.append(cls_down)\n",
    "\n",
    "balanced_data = pd.concat(balanced_samples).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verifica la nuova distribuzione\n",
    "print(\"üìä Distribuzione dopo bilanciamento:\")\n",
    "print(balanced_data['label_tactic_reduced'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=balanced_data['label_tactic_reduced'].value_counts().index,\n",
    "            y=balanced_data['label_tactic_reduced'].value_counts().values,\n",
    "            palette='pastel')\n",
    "plt.title(\"üìä Distribuzione Classi Bilanciate (3-class)\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4: Preparazione dataset per Autoencoder e classificazione\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üèóÔ∏è Inizio preparazione dataset per autoencoder e classificazione (su dataset bilanciato)...\\n\")\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Definizione target e dataset di lavoro\n",
    "# ================================\n",
    "target_multiclass = 'label_tactic_reduced'\n",
    "\n",
    "# Controllo che le colonne esistano\n",
    "required_cols = ['label_binary', 'label_technique', 'label_tactic', target_multiclass]\n",
    "missing_cols = [c for c in required_cols if c not in balanced_data.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"‚ùå Colonne mancanti nel dataset bilanciato: {missing_cols}\")\n",
    "\n",
    "# Label (multiclasse e binaria)\n",
    "y_multiclass = balanced_data[target_multiclass].copy()\n",
    "y_binary = balanced_data['label_binary'].copy()\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Feature set\n",
    "# ================================\n",
    "feature_data = balanced_data.drop(columns=['label_binary', 'label_technique', 'label_tactic'])\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Conversione datetime ‚Üí numerico\n",
    "# ================================\n",
    "datetime_cols = feature_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data[col] = feature_data[col].astype('int64') / 1e9  # secondi Unix\n",
    "\n",
    "# ================================\n",
    "# 4Ô∏è‚É£ Selezione automatica feature numeriche ad alta varianza\n",
    "# ================================\n",
    "# selected_features: lista di feature numeriche con varianza > 0.01 calcolata precedentemente\n",
    "numerical_high_var = [col for col in selected_features if col in feature_data.columns]\n",
    "\n",
    "# ================================\n",
    "# 5Ô∏è‚É£ Frequency Encoding su feature categoriali\n",
    "# ================================\n",
    "cat_features = feature_data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "encoded_data = feature_data.copy()\n",
    "\n",
    "for col in cat_features:\n",
    "    freq = encoded_data[col].value_counts(normalize=True)\n",
    "    encoded_data[col] = encoded_data[col].map(freq)\n",
    "\n",
    "# ================================\n",
    "# 6Ô∏è‚É£ Unione feature numeriche ad alta varianza + categoriali trasformate\n",
    "# ================================\n",
    "encoded_data = encoded_data[numerical_high_var + cat_features]\n",
    "\n",
    "# ================================\n",
    "# 7Ô∏è‚É£ Scaling MinMax per Autoencoder\n",
    "# ================================\n",
    "scaler_auto = MinMaxScaler()\n",
    "X_autoencoder = pd.DataFrame(\n",
    "    scaler_auto.fit_transform(encoded_data),\n",
    "    columns=encoded_data.columns\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 8Ô∏è‚É£ Output finale e riepilogo\n",
    "# ================================\n",
    "print(f\"‚úÖ Dataset pronto per l'autoencoder: {X_autoencoder.shape}\")\n",
    "print(f\"üîπ Numero di feature totali: {X_autoencoder.shape[1]}\")\n",
    "print(f\"üîπ Classi multiclasse bilanciate: {y_multiclass.unique().tolist()}\")\n",
    "print(f\"üîπ Classi binarie: {y_binary.unique().tolist()}\")\n",
    "\n",
    "print(\"\\nüìä Distribuzione finale delle classi (bilanciato):\")\n",
    "display(y_multiclass.value_counts())\n",
    "\n",
    "print(\"‚úÖ Fine preparazione: X_autoencoder, y_multiclass, y_binary pronti per i modelli.\")\n",
    "\n",
    "# ================================\n",
    "# üîπ Spiegazione:\n",
    "# - Solo le feature numeriche con varianza >0.01 sono considerate, perch√© quelle con varianza molto bassa portano poco segnale.\n",
    "# - La selezione √® automatica: la soglia si adatta ai dati, nessuna scelta manuale richiesta.\n",
    "# - Tutte le feature categoriali vengono trasformate in valori numerici tramite frequency encoding.\n",
    "# - L'encoder riceve quindi tutte le feature informative numeriche + tutte le categoriali trasformate.\n",
    "# - Infine, lo scaling MinMax normalizza i valori tra 0 e 1, pronto per l'autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4 Imbalanced: Preparazione dataset (classi sbilanciate con class weights)\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"üèóÔ∏è Inizio preparazione dataset per autoencoder e classificazione (dataset sbilanciato con class weights)...\\n\")\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Definizione target e dataset\n",
    "# ================================\n",
    "target_multiclass_imb = 'label_tactic_reduced'\n",
    "required_cols = ['label_binary', 'label_technique', 'label_tactic', target_multiclass_imb]\n",
    "missing_cols = [c for c in required_cols if c not in data.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"‚ùå Colonne mancanti nel dataset originale: {missing_cols}\")\n",
    "\n",
    "# Target (multiclass ridotto + binario)\n",
    "y_multiclass_imb = data[target_multiclass_imb].copy()\n",
    "y_binary_imb = data['label_binary'].copy()\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Feature set (senza label)\n",
    "# ================================\n",
    "feature_data_imb = data.drop(columns=['label_binary', 'label_technique', 'label_tactic', target_multiclass_imb])\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Conversione datetime ‚Üí numerico (timestamp)\n",
    "# ================================\n",
    "datetime_cols = feature_data_imb.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data_imb[col] = feature_data_imb[col].astype('int64') / 1e9\n",
    "\n",
    "# ================================\n",
    "# 4Ô∏è‚É£ Selezione automatica feature numeriche ad alta varianza\n",
    "# ================================\n",
    "numerical_high_var_imb = [col for col in selected_features if col in feature_data_imb.columns]\n",
    "\n",
    "# ================================\n",
    "# 5Ô∏è‚É£ Frequency Encoding per feature categoriali\n",
    "# ================================\n",
    "cat_features_imb = feature_data_imb.select_dtypes(include=['object','category']).columns.tolist()\n",
    "encoded_data_imb = feature_data_imb.copy()\n",
    "\n",
    "for col in cat_features_imb:\n",
    "    freq = encoded_data_imb[col].value_counts(normalize=True)\n",
    "    encoded_data_imb[col] = encoded_data_imb[col].map(freq)\n",
    "\n",
    "# Mappatura finale feature (utile per pipeline o export)\n",
    "feature_mapping_imb = {col: col for col in numerical_high_var_imb + cat_features_imb}\n",
    "\n",
    "# ================================\n",
    "# 6Ô∏è‚É£ Scaling MinMax\n",
    "# ================================\n",
    "encoded_data_imb = encoded_data_imb[numerical_high_var_imb + cat_features_imb]\n",
    "\n",
    "scaler_auto_imb = MinMaxScaler()\n",
    "X_autoencoder_imb = pd.DataFrame(\n",
    "    scaler_auto_imb.fit_transform(encoded_data_imb),\n",
    "    columns=encoded_data_imb.columns\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 7Ô∏è‚É£ Salvataggio modelli e pesi\n",
    "# ================================\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "joblib.dump(scaler_auto_imb, \"model_data/scaler_auto_imbalanced.pkl\")\n",
    "joblib.dump(feature_mapping_imb, \"model_data/feature_mapping_imbalanced.pkl\")\n",
    "\n",
    "print(f\"‚úÖ Dataset sbilanciato pronto: {X_autoencoder_imb.shape}\")\n",
    "print(f\"üîπ Numero di feature totali: {X_autoencoder_imb.shape[1]}\")\n",
    "print(f\"üîπ Classi multiclass (sbilanciate): {y_multiclass_imb.unique().tolist()}\")\n",
    "\n",
    "# Carica class weights salvati nel blocco precedente\n",
    "class_weights_dict = joblib.load(\"model_data/class_weights_dict.pkl\")\n",
    "print(f\"üîπ Class weights caricati da 'class_weights_dict.pkl'\")\n",
    "\n",
    "print(\"\\nüìä Distribuzione classi (originale, sbilanciata):\")\n",
    "display(y_multiclass_imb.value_counts())\n",
    "\n",
    "print(\"‚úÖ Fine preparazione: X_autoencoder_imb, y_multiclass_imb, y_binary_imb pronti per modelli con class weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0687aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5: Addestramento Autoencoder con logging avanzato,\n",
    "# EarlyStopping, salvataggio best epoch e grafico\n",
    "# ==========================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # riduce warning TF/CUDA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Pulizia memoria Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"üèóÔ∏è Inizio costruzione e training dell'Autoencoder - Bilanciato...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Parametri\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "latent_dim = 16\n",
    "\n",
    "# 2Ô∏è‚É£ Costruzione autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu', name='latent_vector')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3Ô∏è‚É£ EarlyStopping avanzato\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Callback custom per log avanzato\n",
    "class ProgressLogger(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '‚îÅ' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "# 5Ô∏è‚É£ Addestramento\n",
    "logger = ProgressLogger()\n",
    "history = autoencoder.fit(\n",
    "    X_autoencoder,\n",
    "    X_autoencoder,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop, logger]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Autoencoder addestrato con successo.\")\n",
    "\n",
    "# 6Ô∏è‚É£ Recupero best epoch\n",
    "best_epoch_idx = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "best_train_loss = history.history['loss'][best_epoch_idx]\n",
    "best_val_loss = history.history['val_loss'][best_epoch_idx]\n",
    "best_time = logger.epoch_logs[best_epoch_idx]['time_s']\n",
    "\n",
    "print(f\"üèÜ Best epoch: {best_epoch_idx+1}\")\n",
    "print(f\"    Train loss: {best_train_loss:.6f}\")\n",
    "print(f\"    Validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"    Tempo per epoca: {best_time:.2f}s\")\n",
    "\n",
    "# 7Ô∏è‚É£ Estrazione encoder ottimale\n",
    "encoder = Model(inputs=input_layer, outputs=autoencoder.get_layer('latent_vector').output)\n",
    "\n",
    "# 8Ô∏è‚É£ Estrazione embeddings latenti\n",
    "X_latent = encoder.predict(X_autoencoder)\n",
    "X_classifier = pd.DataFrame(X_latent, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "y_classifier = y_multiclass.reset_index(drop=True)\n",
    "\n",
    "# Controllo NaN\n",
    "assert not y_classifier.isna().any(), \"Errore: y_classifier contiene NaN\"\n",
    "\n",
    "print(f\"‚úÖ Embeddings generati: {X_classifier.shape}\")\n",
    "\n",
    "# 9Ô∏è‚É£ Grafico Train vs Validation Loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.axvline(x=best_epoch_idx, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_idx+1})')\n",
    "plt.title(\"Andamento Train/Validation Loss per Epoca\", fontsize=14)\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5 IMBALANCED: Addestramento Autoencoder con logging avanzato\n",
    "# ==========================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # riduce warning TF/CUDA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Pulizia memoria Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"üèóÔ∏è Inizio costruzione e training dell'Autoencoder - Sbilanciato..\")\n",
    "\n",
    "# 1Ô∏è‚É£ Parametri\n",
    "input_dim_imb = X_autoencoder_imb.shape[1]\n",
    "latent_dim_imb = 16\n",
    "\n",
    "# 2Ô∏è‚É£ Costruzione autoencoder\n",
    "input_layer_imb = Input(shape=(input_dim_imb,))\n",
    "encoded_imb = Dense(64, activation='relu')(input_layer_imb)\n",
    "encoded_imb = Dense(32, activation='relu')(encoded_imb)\n",
    "encoded_imb = Dense(latent_dim_imb, activation='relu', name='latent_vector_imb')(encoded_imb)\n",
    "decoded_imb = Dense(32, activation='relu')(encoded_imb)\n",
    "decoded_imb = Dense(64, activation='relu')(decoded_imb)\n",
    "decoded_imb = Dense(input_dim_imb, activation='sigmoid')(decoded_imb)\n",
    "\n",
    "autoencoder_imb = Model(inputs=input_layer_imb, outputs=decoded_imb)\n",
    "autoencoder_imb.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3Ô∏è‚É£ EarlyStopping avanzato\n",
    "early_stop_imb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Callback custom con logging avanzato e tempo per epoca\n",
    "class ProgressLoggerImb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '‚îÅ' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "# 5Ô∏è‚É£ Training\n",
    "logger_imb = ProgressLoggerImb()\n",
    "history_imb = autoencoder_imb.fit(\n",
    "    X_autoencoder_imb,\n",
    "    X_autoencoder_imb,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop_imb, logger_imb]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Autoencoder IMBALANCED addestrato con successo.\")\n",
    "\n",
    "# 6Ô∏è‚É£ Recupero best epoch\n",
    "best_epoch_idx_imb = history_imb.history['val_loss'].index(min(history_imb.history['val_loss']))\n",
    "best_train_loss_imb = history_imb.history['loss'][best_epoch_idx_imb]\n",
    "best_val_loss_imb = history_imb.history['val_loss'][best_epoch_idx_imb]\n",
    "best_time_imb = logger_imb.epoch_logs[best_epoch_idx_imb]['time_s']\n",
    "\n",
    "print(f\"üèÜ Best epoch: {best_epoch_idx_imb+1}\")\n",
    "print(f\"    Train loss: {best_train_loss_imb:.6f}\")\n",
    "print(f\"    Validation loss: {best_val_loss_imb:.6f}\")\n",
    "print(f\"    Tempo per epoca: {best_time_imb:.2f}s\")\n",
    "\n",
    "# 7Ô∏è‚É£ Estrazione encoder ottimale e generazione embeddings\n",
    "encoder_imb = Model(inputs=input_layer_imb, outputs=autoencoder_imb.get_layer('latent_vector_imb').output)\n",
    "\n",
    "X_latent_imb = encoder_imb.predict(X_autoencoder_imb)\n",
    "X_classifier_imb = pd.DataFrame(X_latent_imb, columns=[f'latent_imb_{i}' for i in range(latent_dim_imb)])\n",
    "y_classifier_imb = y_multiclass_imb.reset_index(drop=True)\n",
    "\n",
    "# Controllo NaN\n",
    "assert not y_classifier_imb.isna().any(), \"Errore: y_classifier_imb contiene NaN\"\n",
    "\n",
    "print(f\"‚úÖ Embeddings generati: {X_classifier_imb.shape}\")\n",
    "\n",
    "# 8Ô∏è‚É£ Grafico Train vs Validation Loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history_imb.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_imb.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.axvline(x=best_epoch_idx_imb, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_idx_imb+1})')\n",
    "plt.title(\"Autoencoder IMBALANCED - Andamento Train/Validation Loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 9Ô∏è‚É£ Salvataggio modelli e dataset latenti\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "autoencoder_imb.save(\"model_data/autoencoder_imbalanced.h5\")\n",
    "encoder_imb.save(\"model_data/encoder_imbalanced.h5\")\n",
    "joblib.dump(X_classifier_imb, \"model_data/X_classifier_imbalanced.pkl\")\n",
    "joblib.dump(y_classifier_imb, \"model_data/y_classifier_imbalanced.pkl\")\n",
    "\n",
    "print(\"üíæ Modelli e dataset latenti salvati in 'model_data/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6: Train/Test Split + StandardScaler + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üèóÔ∏è Suddivisione dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Split stratificato\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classifier,\n",
    "    y_classifier,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split completato.\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Percentuali per categoria\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Distribuzione classi nel TRAIN:\")\n",
    "print(train_dist.round(2))\n",
    "print(\"\\nüìä Distribuzione classi nel TEST:\")\n",
    "print(test_dist.round(2))\n",
    "\n",
    "# 3Ô∏è‚É£ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist.index, y=train_dist.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train\")\n",
    "sns.barplot(x=test_dist.index, y=test_dist.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Scaling (solo train, poi test)\n",
    "print(\"\\n‚öôÔ∏è Applicazione StandardScaler sugli embeddings latenti...\")\n",
    "scaler_latent = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_latent.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_latent.transform(X_test), columns=X_test.columns)\n",
    "print(\"‚úÖ Scaling completato.\")\n",
    "\n",
    "# 5Ô∏è‚É£ Visualizzazione confronto pre/post scaling su alcune feature\n",
    "sample_features = X_train.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features):\n",
    "    sns.histplot(X_train[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale\")\n",
    "    sns.histplot(X_train_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset finale pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6 (Imbalanced): Train/Test Split + Scaling + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üèóÔ∏è Suddivisione IMBALANCED dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Split stratificato\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_classifier_imb,\n",
    "    y_classifier_imb,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier_imb,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split completato (Imbalanced).\")\n",
    "print(f\"Train: {X_train_imb.shape}, Test: {X_test_imb.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Percentuali per categoria\n",
    "train_dist_imb = y_train_imb.value_counts(normalize=True) * 100\n",
    "test_dist_imb = y_test_imb.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Distribuzione classi nel TRAIN (Imbalanced):\")\n",
    "print(train_dist_imb.round(2))\n",
    "print(\"\\nüìä Distribuzione classi nel TEST (Imbalanced):\")\n",
    "print(test_dist_imb.round(2))\n",
    "\n",
    "# 3Ô∏è‚É£ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist_imb.index, y=train_dist_imb.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train (Imbalanced)\")\n",
    "sns.barplot(x=test_dist_imb.index, y=test_dist_imb.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test (Imbalanced)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Scaling\n",
    "print(\"\\n‚öôÔ∏è Applicazione StandardScaler sugli embeddings latenti (Imbalanced)...\")\n",
    "scaler_latent_imb = StandardScaler()\n",
    "X_train_imb_scaled = pd.DataFrame(scaler_latent_imb.fit_transform(X_train_imb), columns=X_train_imb.columns)\n",
    "X_test_imb_scaled = pd.DataFrame(scaler_latent_imb.transform(X_test_imb), columns=X_test_imb.columns)\n",
    "print(\"‚úÖ Scaling completato (Imbalanced).\")\n",
    "\n",
    "# 5Ô∏è‚É£ Visualizzazione confronto pre/post scaling\n",
    "sample_features_imb = X_train_imb.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features_imb), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features_imb):\n",
    "    sns.histplot(X_train_imb[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale (Imbalanced)\")\n",
    "    sns.histplot(X_train_imb_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled (Imbalanced)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset IMBALANCED pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_imb_scaled.shape}, Test: {X_test_imb_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 7: Salvataggio dataset, scaler e encoder\n",
    "# ==========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"üíæ Salvataggio dati e oggetti per training...\")\n",
    "\n",
    "# Crea cartella di output se non esiste\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "\n",
    "# 1Ô∏è‚É£ Salvataggio dataset train/test\n",
    "X_train_scaled.to_csv(\"model_data/X_train_balanced.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"model_data/X_test_balanced.csv\", index=False)\n",
    "y_train.to_csv(\"model_data/y_train_balanced.csv\", index=False)\n",
    "y_test.to_csv(\"model_data/y_test_balanced.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset salvati in 'model_data/'\")\n",
    "\n",
    "# 2Ô∏è‚É£ Salvataggio StandardScaler\n",
    "joblib.dump(scaler_latent, \"model_data/scaler_latent.pkl\")\n",
    "print(\"‚úÖ Scaler salvato come 'scaler_latent.pkl'\")\n",
    "\n",
    "# 3Ô∏è‚É£ Salvataggio encoder\n",
    "encoder.save(\"model_data/encoder_best.keras\")\n",
    "print(\"‚úÖ Encoder salvato come 'encoder_best.keras'\")\n",
    "\n",
    "print(\"\\nüéØ Tutti i dati pronti per l'addestramento dei modelli multiclasse!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 7 (Imbalanced): Salvataggio dataset, scaler e encoder\n",
    "# ==========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"üíæ Salvataggio dati e modelli IMBALANCED...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Creazione cartella di output\n",
    "os.makedirs(\"model_data_imbalanced\", exist_ok=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Salvataggio dataset train/test\n",
    "X_train_imb_scaled.to_csv(\"model_data_imbalanced/X_train_imb.csv\", index=False)\n",
    "X_test_imb_scaled.to_csv(\"model_data_imbalanced/X_test_imb.csv\", index=False)\n",
    "y_train_imb.to_csv(\"model_data_imbalanced/y_train_imb.csv\", index=False)\n",
    "y_test_imb.to_csv(\"model_data_imbalanced/y_test_imb.csv\", index=False)\n",
    "print(\"‚úÖ Dataset IMBALANCED salvati in 'model_data_imbalanced/'\")\n",
    "\n",
    "# 3Ô∏è‚É£ Salvataggio scaler\n",
    "joblib.dump(scaler_latent_imb, \"model_data_imbalanced/scaler_latent_imb.pkl\")\n",
    "print(\"‚úÖ Scaler IMBALANCED salvato come 'scaler_latent_imb.pkl'\")\n",
    "\n",
    "# 4Ô∏è‚É£ Salvataggio encoder (dall‚Äôautoencoder imbalanced)\n",
    "encoder_imb.save(\"model_data_imbalanced/encoder_imb_best.keras\")\n",
    "print(\"‚úÖ Encoder IMBALANCED salvato come 'encoder_imb_best.keras'\")\n",
    "\n",
    "print(\"\\nüéØ Tutti i dati e modelli IMBALANCED pronti per il training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
