{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 1: Confronto Attributi Dataset Zeek\n",
    "# Controllo e uniformazione colonne tra UWF-ZeekData22 e UWF-ZeekDataFall22\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Blocchi 1 e 2: Definizione dei percorsi -------------\n",
    "# Blocco 1: Cartella ZeekData22\n",
    "folder_data22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekData22\"\n",
    "\n",
    "# Blocco 2: Cartella ZeekDataFall22\n",
    "folder_datafall22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "\n",
    "# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\n",
    "def load_dataset(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    dfs = []\n",
    "    for f in all_files:\n",
    "        path = os.path.join(folder_path, f)\n",
    "        if f.endswith(\".parquet\"):\n",
    "            dfs.append(pd.read_parquet(path))\n",
    "        elif f.endswith(\".csv\"):\n",
    "            dfs.append(pd.read_csv(path))\n",
    "    if dfs:\n",
    "        # Concateno tutto in un unico dataframe\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------- Blocco 3: Caricamento dataset -------------\n",
    "df_22 = load_dataset(folder_data22)\n",
    "df_fall22 = load_dataset(folder_datafall22)\n",
    "\n",
    "# ------------- Blocco 4: Controllo colonne -------------\n",
    "cols_22 = set(df_22.columns)\n",
    "cols_fall22 = set(df_fall22.columns)\n",
    "\n",
    "# Colonne uniche per ciascun dataset\n",
    "unique_22 = cols_22 - cols_fall22\n",
    "unique_fall22 = cols_fall22 - cols_22\n",
    "common_cols = cols_22 & cols_fall22\n",
    "\n",
    "# ------------- Blocco 5: Stampa report differenze -------------\n",
    "print(\"=== BLOCCO 5: Report differenze tra dataset ===\\n\")\n",
    "print(f\"Totale colonne in ZeekData22: {len(cols_22)}\")\n",
    "print(f\"Totale colonne in ZeekDataFall22: {len(cols_fall22)}\\n\")\n",
    "\n",
    "print(\"Colonne presenti solo in ZeekData22:\")\n",
    "for col in sorted(unique_22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne presenti solo in ZeekDataFall22:\")\n",
    "for col in sorted(unique_fall22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne comuni a entrambi i dataset:\")\n",
    "for col in sorted(common_cols):\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 1: Data Cleaning su ZeekDataFall22\n",
    "# Scopo:\n",
    "# 1. Rimuovere duplicati: log di connessioni identici possono alterare l'addestramento\n",
    "# 2. Imputare valori mancanti: evitare errori negli algoritmi ML e ridurre bias\n",
    "# 3. Aggregazioni (session-level features): combinare più record di connessione in metriche significative\n",
    "#    come totale pacchetti per sessione o durata media, per rendere il modello più robusto\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella contenente i file parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# Caricamento dati in un unico DataFrame\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe prima del cleaning: {len(data)}\")\n",
    "print(f\"Totale colonne: {data.shape[1]}\")\n",
    "\n",
    "# ==========================\n",
    "# 1. Rimozione duplicati\n",
    "# ==========================\n",
    "duplicates_before = data.duplicated().sum()\n",
    "data = data.drop_duplicates()\n",
    "duplicates_after = data.duplicated().sum()\n",
    "print(f\"Duplicati trovati e rimossi: {duplicates_before - duplicates_after}\")\n",
    "\n",
    "# ==========================\n",
    "# 2. Imputazione valori mancanti\n",
    "# ==========================\n",
    "# Strategia: \n",
    "# - numeriche → media\n",
    "# - categoriali → moda\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "imputation_count = 0\n",
    "\n",
    "for col in num_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "for col in cat_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "print(f\"Totale valori imputati: {imputation_count}\")\n",
    "\n",
    "# ==========================\n",
    "# 3. Aggregazioni (session-level features)\n",
    "# ==========================\n",
    "# Spiegazione:\n",
    "# - Le connessioni multiple tra gli stessi host possono essere aggregate in sessioni\n",
    "# - Feature utili: totale pacchetti, byte totali, durata media\n",
    "# - Questo permette al modello di avere una visione più globale del comportamento di rete\n",
    "# - Qui consideriamo 'uid' come identificativo della sessione\n",
    "\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# Unione delle aggregazioni col dataframe originale per eventuali altre feature\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "\n",
    "# Mostriamo le prime righe del dataframe pulito\n",
    "print(\"Prime righe del dataset dopo cleaning e aggregazioni:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"⚠️ Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Distribuzione prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"📊 Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione righe con label_technique 'none' -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"✅ Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\n📊 Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "# Aggiornamento grafico dopo rimozione\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"📊 Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento mapping tactic se esiste -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\n📊 Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 2: Controllo valori nulli e riepilogo colonne/feature\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"✅ Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"⚠️ Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\n📊 Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\n🔹 Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\n🔹 Feature categoriali:\")\n",
    "print(cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta con skew/kurtosis\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Feature numeriche candidate\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "var_threshold = 0.01\n",
    "selected_features = variance[variance > var_threshold].index.tolist()\n",
    "print(f\"✅ Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "outlier_examples = {}\n",
    "for col in selected_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    outliers = data[(data[col]<lower) | (data[col]>upper)]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "    outlier_examples[col] = outliers[col].head(5).tolist()\n",
    "    data[f'{col}_outlier'] = ((data[col]<lower) | (data[col]>upper)).astype(int)\n",
    "\n",
    "print(\"⚠️ Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "print(\"\\n📌 Esempi di outlier (primi 5 per feature):\")\n",
    "for col, examples in outlier_examples.items():\n",
    "    print(f\"{col}: {examples}\")\n",
    "\n",
    "# ================================\n",
    "# Step 4: Visualizzazione distribuzioni prima della trasformazione\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col], bins=50, kde=True, color='lightcoral')\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Step 5: Lista feature finali per scaling\n",
    "# ================================\n",
    "numeric_for_scaling = selected_features\n",
    "print(f\"\\n📌 Feature numeriche finali pronte per scaling/normalizzazione: {numeric_for_scaling}\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Calcolo skew/kurtosis pre-transform\n",
    "# ================================\n",
    "pre_skew_kurt = pd.DataFrame({\n",
    "    'skew_pre': data[numeric_for_scaling].skew(),\n",
    "    'kurtosis_pre': data[numeric_for_scaling].kurtosis()\n",
    "})\n",
    "\n",
    "# ==========================================================\n",
    "# Step 7: Trasformazione outlier (Winsorization + log)\n",
    "# ==========================================================\n",
    "print(\"🏗️ Applicazione trasformazioni per gestire outlier sulle feature numeriche...\\n\")\n",
    "for col in numeric_for_scaling:\n",
    "    # Winsorization ai percentili 1% e 99%\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    \n",
    "    # Trasformazione log per ridurre asimmetria\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    \n",
    "    # Sostituzione inf/-inf con NaN\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ================================\n",
    "# Step 8: Calcolo skew/kurtosis post-transform\n",
    "# ================================\n",
    "post_skew_kurt = pd.DataFrame({\n",
    "    'skew_post': data[numeric_for_scaling].skew(),\n",
    "    'kurtosis_post': data[numeric_for_scaling].kurtosis()\n",
    "})\n",
    "\n",
    "skew_kurt_comparison = pre_skew_kurt.join(post_skew_kurt)\n",
    "\n",
    "# ================================\n",
    "# Step 9: Report finale post-transform\n",
    "# ================================\n",
    "print(\"\\n📄 REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "\n",
    "# Statistiche descrittive\n",
    "print(\"🔹 Statistiche descrittive delle feature numeriche:\")\n",
    "display(data[numeric_for_scaling].describe().T)\n",
    "\n",
    "# Confronto outlier pre/post\n",
    "original_outliers_count = pd.Series(outlier_summary)\n",
    "post_outliers_count = data[[f\"{col}_outlier\" for col in numeric_for_scaling]].sum()\n",
    "outlier_comparison = pd.DataFrame({\n",
    "    'Outlier_originali': original_outliers_count,\n",
    "    'Outlier_flag_post': post_outliers_count\n",
    "})\n",
    "print(\"\\n🔹 Confronto numero outlier (prima vs flag post-Winsorization):\")\n",
    "display(outlier_comparison)\n",
    "\n",
    "# Confronto skew/kurtosis\n",
    "print(\"\\n🔹 Confronto skewness e kurtosis (pre vs post trasformazione):\")\n",
    "display(skew_kurt_comparison)\n",
    "\n",
    "# Motivazione trasformazione\n",
    "print(\"\\n📝 Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere la distribuzione e l'apprendimento del modello.\\n\"\n",
    "    \"- Winsorization: limita valori estremi ai percentili 1% e 99%, riducendo l'impatto degli outlier senza rimuoverli.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria delle distribuzioni skewed, migliora stabilità numerica e scaling.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni più compatte, outlier gestiti e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 10: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(numeric_for_scaling)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(numeric_for_scaling, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"📊 Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"📊 Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"📊 Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"⚖️ Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4: Preparazione dataset per Autoencoder e classificazione\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"🏗️ Inizio preparazione dataset per autoencoder e classificazione...\\n\")\n",
    "\n",
    "# 1️⃣ Separiamo le label (non incluse nel training dell’autoencoder)\n",
    "target_multiclass = 'label_tactic_reduced'\n",
    "y_multiclass = data[target_multiclass].copy()\n",
    "y_binary = data['label_binary'].copy()\n",
    "\n",
    "# 2️⃣ Creiamo il dataframe solo con le feature\n",
    "feature_data = data.drop(columns=['label_binary', 'label_technique', 'label_tactic'])\n",
    "\n",
    "# 3️⃣ Conversione datetime → numerico (se presenti)\n",
    "datetime_cols = feature_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data[col] = feature_data[col].astype('int64') / 1e9\n",
    "\n",
    "# 4️⃣ Frequency Encoding su categoriali\n",
    "cat_features = feature_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_data = feature_data.copy()\n",
    "for col in cat_features:\n",
    "    freq = encoded_data[col].value_counts(normalize=True)\n",
    "    encoded_data[col] = encoded_data[col].map(freq)\n",
    "\n",
    "# 5️⃣ Scaling MinMax per l’autoencoder\n",
    "scaler_auto = MinMaxScaler()\n",
    "X_autoencoder = pd.DataFrame(scaler_auto.fit_transform(encoded_data), columns=encoded_data.columns)\n",
    "\n",
    "print(f\"✅ Dataset pronto per l'autoencoder: {X_autoencoder.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5: Addestramento Autoencoder con logging avanzato, EarlyStopping e grafico live\n",
    "# ==========================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🏗️ Inizio costruzione e training dell'Autoencoder...\")\n",
    "\n",
    "# 1️⃣ Parametri\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "latent_dim = 16\n",
    "\n",
    "# 2️⃣ Costruzione autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu', name='latent_vector')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3️⃣ EarlyStopping avanzato\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 4️⃣ Callback custom con barra, tabella e grafico live\n",
    "class ProgressLoggerPlot(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "        plt.ion()\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4))\n",
    "        self.ax.set_xlabel(\"Epoch\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        self.ax.set_title(\"Training / Validation Loss\")\n",
    "        self.train_line, = self.ax.plot([], [], 'b-', label='train_loss')\n",
    "        self.val_line, = self.ax.plot([], [], 'r-', label='val_loss')\n",
    "        self.ax.legend()\n",
    "        self.ax.grid(True)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "\n",
    "        # Barra stile Keras\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '━' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "        # Aggiorna grafico live\n",
    "        epochs = [log['epoch'] for log in self.epoch_logs]\n",
    "        train_losses = [log['loss'] for log in self.epoch_logs]\n",
    "        val_losses = [log['val_loss'] for log in self.epoch_logs]\n",
    "        self.train_line.set_data(epochs, train_losses)\n",
    "        self.val_line.set_data(epochs, val_losses)\n",
    "        self.ax.set_xlim(1, max(epochs)+1)\n",
    "        self.ax.set_ylim(0, max(max(train_losses), max(val_losses))*1.1)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "\n",
    "# 5️⃣ Addestramento\n",
    "history = autoencoder.fit(\n",
    "    X_autoencoder,\n",
    "    X_autoencoder,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,  # silenzioso, logging tramite callback\n",
    "    callbacks=[early_stop, ProgressLoggerPlot()]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Autoencoder addestrato con successo.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Estrazione encoder ottimale\n",
    "# -----------------------------\n",
    "encoder = Model(inputs=input_layer, outputs=autoencoder.get_layer('latent_vector').output)\n",
    "encoder.save(\"encoder_best.h5\")\n",
    "\n",
    "# 7️⃣ Estrazione embeddings latenti\n",
    "X_latent = encoder.predict(X_autoencoder)\n",
    "X_classifier = pd.DataFrame(X_latent, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "y_classifier = y_multiclass.reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Embeddings generati: {X_classifier.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f880bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6: Train/Test Split + StandardScaler + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🏗️ Suddivisione dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1️⃣ Split stratificato per mantenere proporzioni\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classifier,\n",
    "    y_classifier,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Split completato.\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 2️⃣ Percentuali per categoria\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\n📊 Distribuzione classi nel TRAIN:\")\n",
    "print(train_dist.round(2))\n",
    "print(\"\\n📊 Distribuzione classi nel TEST:\")\n",
    "print(test_dist.round(2))\n",
    "\n",
    "# 3️⃣ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist.index, y=train_dist.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train\")\n",
    "sns.barplot(x=test_dist.index, y=test_dist.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4️⃣ Scaling (solo train, poi trasformiamo test)\n",
    "print(\"\\n⚙️ Applicazione StandardScaler sugli embeddings latenti...\")\n",
    "scaler_latent = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_latent.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_latent.transform(X_test), columns=X_test.columns)\n",
    "print(\"✅ Scaling completato.\")\n",
    "\n",
    "# 5️⃣ Visualizzazione confronto pre/post scaling su alcune feature\n",
    "sample_features = X_train.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features):\n",
    "    sns.histplot(X_train[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale\")\n",
    "    sns.histplot(X_train_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Dataset finale pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
