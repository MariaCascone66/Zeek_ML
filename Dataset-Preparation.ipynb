{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 1: Confronto Attributi Dataset Zeek\n",
    "# Controllo e uniformazione colonne tra UWF-ZeekData22 e UWF-ZeekDataFall22\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Blocchi 1 e 2: Definizione dei percorsi -------------\n",
    "# Blocco 1: Cartella ZeekData22\n",
    "folder_data22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekData22\"\n",
    "\n",
    "# Blocco 2: Cartella ZeekDataFall22\n",
    "folder_datafall22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "\n",
    "# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\n",
    "def load_dataset(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    dfs = []\n",
    "    for f in all_files:\n",
    "        path = os.path.join(folder_path, f)\n",
    "        if f.endswith(\".parquet\"):\n",
    "            dfs.append(pd.read_parquet(path))\n",
    "        elif f.endswith(\".csv\"):\n",
    "            dfs.append(pd.read_csv(path))\n",
    "    if dfs:\n",
    "        # Concateno tutto in un unico dataframe\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------- Blocco 3: Caricamento dataset -------------\n",
    "df_22 = load_dataset(folder_data22)\n",
    "df_fall22 = load_dataset(folder_datafall22)\n",
    "\n",
    "# ------------- Blocco 4: Controllo colonne -------------\n",
    "cols_22 = set(df_22.columns)\n",
    "cols_fall22 = set(df_fall22.columns)\n",
    "\n",
    "# Colonne uniche per ciascun dataset\n",
    "unique_22 = cols_22 - cols_fall22\n",
    "unique_fall22 = cols_fall22 - cols_22\n",
    "common_cols = cols_22 & cols_fall22\n",
    "\n",
    "# ------------- Blocco 5: Stampa report differenze -------------\n",
    "print(\"=== BLOCCO 5: Report differenze tra dataset ===\\n\")\n",
    "print(f\"Totale colonne in ZeekData22: {len(cols_22)}\")\n",
    "print(f\"Totale colonne in ZeekDataFall22: {len(cols_fall22)}\\n\")\n",
    "\n",
    "print(\"Colonne presenti solo in ZeekData22:\")\n",
    "for col in sorted(unique_22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne presenti solo in ZeekDataFall22:\")\n",
    "for col in sorted(unique_fall22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne comuni a entrambi i dataset:\")\n",
    "for col in sorted(common_cols):\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 1: Data Cleaning su ZeekDataFall22\n",
    "# Scopo:\n",
    "# 1. Rimuovere duplicati: log di connessioni identici possono alterare l'addestramento\n",
    "# 2. Imputare valori mancanti: evitare errori negli algoritmi ML e ridurre bias\n",
    "# 3. Aggregazioni (session-level features): combinare pi√π record di connessione in metriche significative\n",
    "#    come totale pacchetti per sessione o durata media, per rendere il modello pi√π robusto\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella contenente i file parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# Caricamento dati in un unico DataFrame\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe prima del cleaning: {len(data)}\")\n",
    "print(f\"Totale colonne: {data.shape[1]}\")\n",
    "\n",
    "# ==========================\n",
    "# 1. Rimozione duplicati\n",
    "# ==========================\n",
    "duplicates_before = data.duplicated().sum()\n",
    "data = data.drop_duplicates()\n",
    "duplicates_after = data.duplicated().sum()\n",
    "print(f\"Duplicati trovati e rimossi: {duplicates_before - duplicates_after}\")\n",
    "\n",
    "# ==========================\n",
    "# 2. Imputazione valori mancanti\n",
    "# ==========================\n",
    "# Strategia: \n",
    "# - numeriche ‚Üí media\n",
    "# - categoriali ‚Üí moda\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "imputation_count = 0\n",
    "\n",
    "for col in num_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "for col in cat_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "print(f\"Totale valori imputati: {imputation_count}\")\n",
    "\n",
    "# ==========================\n",
    "# 3. Aggregazioni (session-level features)\n",
    "# ==========================\n",
    "# Spiegazione:\n",
    "# - Le connessioni multiple tra gli stessi host possono essere aggregate in sessioni\n",
    "# - Feature utili: totale pacchetti, byte totali, durata media\n",
    "# - Questo permette al modello di avere una visione pi√π globale del comportamento di rete\n",
    "# - Qui consideriamo 'uid' come identificativo della sessione\n",
    "\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# Unione delle aggregazioni col dataframe originale per eventuali altre feature\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "\n",
    "# Mostriamo le prime righe del dataframe pulito\n",
    "print(\"Prime righe del dataset dopo cleaning e aggregazioni:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"‚ö†Ô∏è Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Distribuzione prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione righe con label_technique 'none' -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"‚úÖ Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\nüìä Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "# Aggiornamento grafico dopo rimozione\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento mapping tactic se esiste -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\nüìä Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCCO 2: Controllo valori nulli e riepilogo colonne/feature\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"‚úÖ Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\nüìä Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\nüîπ Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\nüîπ Feature categoriali:\")\n",
    "print(cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta con skew/kurtosis\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Feature numeriche candidate\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "var_threshold = 0.01\n",
    "selected_features = variance[variance > var_threshold].index.tolist()\n",
    "print(f\"‚úÖ Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "outlier_examples = {}\n",
    "for col in selected_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    outliers = data[(data[col]<lower) | (data[col]>upper)]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "    outlier_examples[col] = outliers[col].head(5).tolist()\n",
    "    data[f'{col}_outlier'] = ((data[col]<lower) | (data[col]>upper)).astype(int)\n",
    "\n",
    "print(\"‚ö†Ô∏è Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "print(\"\\nüìå Esempi di outlier (primi 5 per feature):\")\n",
    "for col, examples in outlier_examples.items():\n",
    "    print(f\"{col}: {examples}\")\n",
    "\n",
    "# ================================\n",
    "# Step 4: Visualizzazione distribuzioni prima della trasformazione\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col], bins=50, kde=True, color='lightcoral')\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Step 5: Lista feature finali per scaling\n",
    "# ================================\n",
    "numeric_for_scaling = selected_features\n",
    "print(f\"\\nüìå Feature numeriche finali pronte per scaling/normalizzazione: {numeric_for_scaling}\")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Calcolo skew/kurtosis pre-transform\n",
    "# ================================\n",
    "pre_skew_kurt = pd.DataFrame({\n",
    "    'skew_pre': data[numeric_for_scaling].skew(),\n",
    "    'kurtosis_pre': data[numeric_for_scaling].kurtosis()\n",
    "})\n",
    "\n",
    "# ==========================================================\n",
    "# Step 7: Trasformazione outlier (Winsorization + log)\n",
    "# ==========================================================\n",
    "print(\"üèóÔ∏è Applicazione trasformazioni per gestire outlier sulle feature numeriche...\\n\")\n",
    "for col in numeric_for_scaling:\n",
    "    # Winsorization ai percentili 1% e 99%\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    \n",
    "    # Trasformazione log per ridurre asimmetria\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    \n",
    "    # Sostituzione inf/-inf con NaN\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ================================\n",
    "# Step 8: Calcolo skew/kurtosis post-transform\n",
    "# ================================\n",
    "post_skew_kurt = pd.DataFrame({\n",
    "    'skew_post': data[numeric_for_scaling].skew(),\n",
    "    'kurtosis_post': data[numeric_for_scaling].kurtosis()\n",
    "})\n",
    "\n",
    "skew_kurt_comparison = pre_skew_kurt.join(post_skew_kurt)\n",
    "\n",
    "# ================================\n",
    "# Step 9: Report finale post-transform\n",
    "# ================================\n",
    "print(\"\\nüìÑ REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "\n",
    "# Statistiche descrittive\n",
    "print(\"üîπ Statistiche descrittive delle feature numeriche:\")\n",
    "display(data[numeric_for_scaling].describe().T)\n",
    "\n",
    "# Confronto outlier pre/post\n",
    "original_outliers_count = pd.Series(outlier_summary)\n",
    "post_outliers_count = data[[f\"{col}_outlier\" for col in numeric_for_scaling]].sum()\n",
    "outlier_comparison = pd.DataFrame({\n",
    "    'Outlier_originali': original_outliers_count,\n",
    "    'Outlier_flag_post': post_outliers_count\n",
    "})\n",
    "print(\"\\nüîπ Confronto numero outlier (prima vs flag post-Winsorization):\")\n",
    "display(outlier_comparison)\n",
    "\n",
    "# Confronto skew/kurtosis\n",
    "print(\"\\nüîπ Confronto skewness e kurtosis (pre vs post trasformazione):\")\n",
    "display(skew_kurt_comparison)\n",
    "\n",
    "# Motivazione trasformazione\n",
    "print(\"\\nüìù Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere la distribuzione e l'apprendimento del modello.\\n\"\n",
    "    \"- Winsorization: limita valori estremi ai percentili 1% e 99%, riducendo l'impatto degli outlier senza rimuoverli.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria delle distribuzioni skewed, migliora stabilit√† numerica e scaling.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni pi√π compatte, outlier gestiti e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 10: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(numeric_for_scaling)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(numeric_for_scaling, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"üìä Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"üìä Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"‚öñÔ∏è Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4: Preparazione dataset per Autoencoder e classificazione\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üèóÔ∏è Inizio preparazione dataset per autoencoder e classificazione...\\n\")\n",
    "\n",
    "# 1Ô∏è‚É£ Separiamo le label (non incluse nel training dell‚Äôautoencoder)\n",
    "target_multiclass = 'label_tactic_reduced'\n",
    "y_multiclass = data[target_multiclass].copy()\n",
    "y_binary = data['label_binary'].copy()\n",
    "\n",
    "# 2Ô∏è‚É£ Creiamo il dataframe solo con le feature\n",
    "feature_data = data.drop(columns=['label_binary', 'label_technique', 'label_tactic'])\n",
    "\n",
    "# 3Ô∏è‚É£ Conversione datetime ‚Üí numerico (se presenti)\n",
    "datetime_cols = feature_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data[col] = feature_data[col].astype('int64') / 1e9\n",
    "\n",
    "# 4Ô∏è‚É£ Frequency Encoding su categoriali\n",
    "cat_features = feature_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_data = feature_data.copy()\n",
    "for col in cat_features:\n",
    "    freq = encoded_data[col].value_counts(normalize=True)\n",
    "    encoded_data[col] = encoded_data[col].map(freq)\n",
    "\n",
    "# 5Ô∏è‚É£ Scaling MinMax per l‚Äôautoencoder\n",
    "scaler_auto = MinMaxScaler()\n",
    "X_autoencoder = pd.DataFrame(scaler_auto.fit_transform(encoded_data), columns=encoded_data.columns)\n",
    "\n",
    "print(f\"‚úÖ Dataset pronto per l'autoencoder: {X_autoencoder.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5: Addestramento Autoencoder con logging avanzato, EarlyStopping e grafico live\n",
    "# ==========================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üèóÔ∏è Inizio costruzione e training dell'Autoencoder...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Parametri\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "latent_dim = 16\n",
    "\n",
    "# 2Ô∏è‚É£ Costruzione autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu', name='latent_vector')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3Ô∏è‚É£ EarlyStopping avanzato\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Callback custom con barra, tabella e grafico live\n",
    "class ProgressLoggerPlot(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "        plt.ion()\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 4))\n",
    "        self.ax.set_xlabel(\"Epoch\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        self.ax.set_title(\"Training / Validation Loss\")\n",
    "        self.train_line, = self.ax.plot([], [], 'b-', label='train_loss')\n",
    "        self.val_line, = self.ax.plot([], [], 'r-', label='val_loss')\n",
    "        self.ax.legend()\n",
    "        self.ax.grid(True)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "\n",
    "        # Barra stile Keras\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '‚îÅ' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "        # Aggiorna grafico live\n",
    "        epochs = [log['epoch'] for log in self.epoch_logs]\n",
    "        train_losses = [log['loss'] for log in self.epoch_logs]\n",
    "        val_losses = [log['val_loss'] for log in self.epoch_logs]\n",
    "        self.train_line.set_data(epochs, train_losses)\n",
    "        self.val_line.set_data(epochs, val_losses)\n",
    "        self.ax.set_xlim(1, max(epochs)+1)\n",
    "        self.ax.set_ylim(0, max(max(train_losses), max(val_losses))*1.1)\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "\n",
    "# 5Ô∏è‚É£ Addestramento\n",
    "history = autoencoder.fit(\n",
    "    X_autoencoder,\n",
    "    X_autoencoder,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,  # silenzioso, logging tramite callback\n",
    "    callbacks=[early_stop, ProgressLoggerPlot()]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Autoencoder addestrato con successo.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ Estrazione encoder ottimale\n",
    "# -----------------------------\n",
    "encoder = Model(inputs=input_layer, outputs=autoencoder.get_layer('latent_vector').output)\n",
    "encoder.save(\"encoder_best.h5\")\n",
    "\n",
    "# 7Ô∏è‚É£ Estrazione embeddings latenti\n",
    "X_latent = encoder.predict(X_autoencoder)\n",
    "X_classifier = pd.DataFrame(X_latent, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "y_classifier = y_multiclass.reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Embeddings generati: {X_classifier.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f880bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6: Train/Test Split + StandardScaler + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üèóÔ∏è Suddivisione dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Split stratificato per mantenere proporzioni\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classifier,\n",
    "    y_classifier,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split completato.\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Percentuali per categoria\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Distribuzione classi nel TRAIN:\")\n",
    "print(train_dist.round(2))\n",
    "print(\"\\nüìä Distribuzione classi nel TEST:\")\n",
    "print(test_dist.round(2))\n",
    "\n",
    "# 3Ô∏è‚É£ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist.index, y=train_dist.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train\")\n",
    "sns.barplot(x=test_dist.index, y=test_dist.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Scaling (solo train, poi trasformiamo test)\n",
    "print(\"\\n‚öôÔ∏è Applicazione StandardScaler sugli embeddings latenti...\")\n",
    "scaler_latent = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_latent.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_latent.transform(X_test), columns=X_test.columns)\n",
    "print(\"‚úÖ Scaling completato.\")\n",
    "\n",
    "# 5Ô∏è‚É£ Visualizzazione confronto pre/post scaling su alcune feature\n",
    "sample_features = X_train.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features):\n",
    "    sns.histplot(X_train[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale\")\n",
    "    sns.histplot(X_train_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset finale pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
