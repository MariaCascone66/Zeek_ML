{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25ae68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Blocco 2: Cartella ZeekDataFall22\u001b[39;00m\n\u001b[32m     13\u001b[39m folder_datafall22 = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmaria\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mZeek_ML\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUWF-ZeekDataFall22\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfolder_data22\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m folder_datafall22.empty:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâš ï¸ Uno dei dataset Ã¨ vuoto. Controlla i percorsi o i formati dei file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 1: Confronto Attributi Dataset Zeek\n",
    "# Controllo e uniformazione colonne tra UWF-ZeekData22 e UWF-ZeekDataFall22\n",
    "# ==========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Blocchi 1 e 2: Definizione dei percorsi -------------\n",
    "# Blocco 1: Cartella ZeekData22\n",
    "folder_data22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekData22\"\n",
    "\n",
    "# Blocco 2: Cartella ZeekDataFall22\n",
    "folder_datafall22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "\n",
    "# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\n",
    "def load_dataset(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    dfs = []\n",
    "    for f in all_files:\n",
    "        path = os.path.join(folder_path, f)\n",
    "        if f.endswith(\".parquet\"):\n",
    "            dfs.append(pd.read_parquet(path))\n",
    "        elif f.endswith(\".csv\"):\n",
    "            dfs.append(pd.read_csv(path))\n",
    "    if dfs:\n",
    "        # Concateno tutto in un unico dataframe\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------- Blocco 3: Caricamento dataset -------------\n",
    "df_22 = load_dataset(folder_data22)\n",
    "df_fall22 = load_dataset(folder_datafall22)\n",
    "\n",
    "# ------------- Blocco 4: Controllo colonne -------------\n",
    "cols_22 = set(df_22.columns)\n",
    "cols_fall22 = set(df_fall22.columns)\n",
    "\n",
    "# Colonne uniche per ciascun dataset\n",
    "unique_22 = cols_22 - cols_fall22\n",
    "unique_fall22 = cols_fall22 - cols_22\n",
    "common_cols = cols_22 & cols_fall22\n",
    "\n",
    "# ------------- Blocco 5: Stampa report differenze -------------\n",
    "print(\"=== BLOCCO 5: Report differenze tra dataset ===\\n\")\n",
    "print(f\"Totale colonne in ZeekData22: {len(cols_22)}\")\n",
    "print(f\"Totale colonne in ZeekDataFall22: {len(cols_fall22)}\\n\")\n",
    "\n",
    "print(\"Colonne presenti solo in ZeekData22:\")\n",
    "for col in sorted(unique_22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne presenti solo in ZeekDataFall22:\")\n",
    "for col in sorted(unique_fall22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne comuni a entrambi i dataset:\")\n",
    "for col in sorted(common_cols):\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2aâ€“2b: Data Cleaning e Aggregazione (ZeekDataFall22)\n",
    "# ==========================================================\n",
    "# Scopo:\n",
    "# 1. Rimuovere duplicati\n",
    "# 2. Imputare valori mancanti\n",
    "# 3. Aggregare per sessione (uid) con feature significative\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella contenente i file parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# Caricamento dati in un unico DataFrame\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe prima del cleaning: {len(data)}\")\n",
    "print(f\"Totale colonne: {data.shape[1]}\")\n",
    "\n",
    "# ==========================\n",
    "# 1. Rimozione duplicati\n",
    "# ==========================\n",
    "duplicates_before = data.duplicated().sum()\n",
    "data = data.drop_duplicates()\n",
    "print(f\"Duplicati trovati e rimossi: {duplicates_before}\")\n",
    "\n",
    "# ==========================\n",
    "# 2. Imputazione valori mancanti\n",
    "# ==========================\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "imputation_count = 0\n",
    "for col in num_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "for col in cat_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mode()[0] if not data[col].mode().empty else 'unknown', inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "print(f\"Totale valori imputati: {imputation_count}\")\n",
    "\n",
    "# ==========================\n",
    "# 3. Aggregazioni (session-level features)\n",
    "# ==========================\n",
    "print(\"ğŸ§¹ Pulizia e aggregazione dati ZeekDataFall22...\\n\")\n",
    "\n",
    "if 'uid' not in data.columns:\n",
    "    print(\"âš ï¸ Colonna 'uid' mancante: creata automaticamente come indice numerico sequenziale.\")\n",
    "    data['uid'] = range(len(data))\n",
    "\n",
    "# Aggregazione per uid\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# Merge con il dataset originale\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "\n",
    "print(\"Prime righe del dataset dopo cleaning e aggregazioni:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2c: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Controllo colonna obbligatoria\n",
    "if 'label_technique' not in data.columns:\n",
    "    raise KeyError(\"âš ï¸ Manca la colonna 'label_technique' nel dataset caricato.\")\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"âš ï¸ Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Grafico prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"ğŸ“Š Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione benigni -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"âœ… Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\nğŸ“Š Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"ğŸ“Š Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento tactic -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\nğŸ“Š Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3a: Controllo valori nulli e riepilogo colonne/feature\n",
    "# ==========================================================\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"âœ… Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\nğŸ“Š Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\nğŸ”¹ Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\nğŸ”¹ Feature categoriali:\")\n",
    "print(cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Selezione feature numeriche\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "selected_features = variance[variance > 0.01].index.tolist()\n",
    "print(f\"âœ… Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "for col in selected_features:\n",
    "    Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    outlier_summary[col] = ((data[col]<lower) | (data[col]>upper)).sum()\n",
    "print(\"âš ï¸ Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Trasformazione outlier (Winsorization + log)\n",
    "# ================================\n",
    "print(\"ğŸ—ï¸ Applicazione trasformazioni per gestire outlier...\\n\")\n",
    "for col in selected_features:\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ğŸ”§ Fix: rimozione eventuali NaN residui\n",
    "data = data.dropna(subset=selected_features)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Analisi post-transform\n",
    "# ================================\n",
    "print(\"\\nğŸ“„ REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "print(\"ğŸ”¹ Statistiche descrittive:\")\n",
    "display(data[selected_features].describe().T)\n",
    "\n",
    "print(\"\\nğŸ“ Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere le distribuzioni.\\n\"\n",
    "    \"- Winsorization: limita i valori ai percentili 1% e 99%, riducendo l'impatto degli outlier.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria e migliora la stabilitÃ  numerica.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni piÃ¹ compatte e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ğŸ“Š Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"ğŸ“Š Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"ğŸ“Š Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"âš–ï¸ Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3f: Creazione dataset bilanciato (3 classi principali)\n",
    "# ==========================================================\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"âš–ï¸ Creazione dataset bilanciato tra le tre classi principali...\\n\")\n",
    "\n",
    "# Filtra solo le 3 classi principali\n",
    "balanced_data = data[data['label_tactic_reduced'].isin(['Resource Development', 'Reconnaissance', 'Discovery'])]\n",
    "\n",
    "# Trova la classe piÃ¹ piccola\n",
    "min_count = balanced_data['label_tactic_reduced'].value_counts().min()\n",
    "\n",
    "# Esegui sottocampionamento per bilanciare\n",
    "balanced_samples = []\n",
    "for cls in ['Resource Development', 'Reconnaissance', 'Discovery']:\n",
    "    cls_df = balanced_data[balanced_data['label_tactic_reduced'] == cls]\n",
    "    cls_down = resample(cls_df, replace=False, n_samples=min_count, random_state=42)\n",
    "    balanced_samples.append(cls_down)\n",
    "\n",
    "balanced_data = pd.concat(balanced_samples).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verifica la nuova distribuzione\n",
    "print(\"ğŸ“Š Distribuzione dopo bilanciamento:\")\n",
    "print(balanced_data['label_tactic_reduced'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=balanced_data['label_tactic_reduced'].value_counts().index,\n",
    "            y=balanced_data['label_tactic_reduced'].value_counts().values,\n",
    "            palette='pastel')\n",
    "plt.title(\"ğŸ“Š Distribuzione Classi Bilanciate (3-class)\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4: Preparazione dataset per Autoencoder e classificazione\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"ğŸ—ï¸ Inizio preparazione dataset per autoencoder e classificazione (su dataset bilanciato)...\\n\")\n",
    "\n",
    "# ================================\n",
    "# 1ï¸âƒ£ Definizione target e dataset di lavoro\n",
    "# ================================\n",
    "target_multiclass = 'label_tactic_reduced'\n",
    "\n",
    "# Controllo che le colonne esistano\n",
    "required_cols = ['label_binary', 'label_technique', 'label_tactic', target_multiclass]\n",
    "missing_cols = [c for c in required_cols if c not in balanced_data.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"âŒ Colonne mancanti nel dataset bilanciato: {missing_cols}\")\n",
    "\n",
    "# Label (multiclasse e binaria)\n",
    "y_multiclass = balanced_data[target_multiclass].copy()\n",
    "y_binary = balanced_data['label_binary'].copy()\n",
    "\n",
    "# ================================\n",
    "# 2ï¸âƒ£ Feature set\n",
    "# ================================\n",
    "feature_data = balanced_data.drop(columns=['label_binary', 'label_technique', 'label_tactic'])\n",
    "\n",
    "# ================================\n",
    "# 3ï¸âƒ£ Conversione datetime â†’ numerico\n",
    "# ================================\n",
    "datetime_cols = feature_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data[col] = feature_data[col].astype('int64') / 1e9  # secondi Unix\n",
    "\n",
    "# ================================\n",
    "# 4ï¸âƒ£ Frequency Encoding su categoriali\n",
    "# ================================\n",
    "cat_features = feature_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_data = feature_data.copy()\n",
    "\n",
    "for col in cat_features:\n",
    "    freq = encoded_data[col].value_counts(normalize=True)\n",
    "    encoded_data[col] = encoded_data[col].map(freq)\n",
    "\n",
    "# ================================\n",
    "# 5ï¸âƒ£ Scaling MinMax per lâ€™Autoencoder\n",
    "# ================================\n",
    "scaler_auto = MinMaxScaler()\n",
    "X_autoencoder = pd.DataFrame(\n",
    "    scaler_auto.fit_transform(encoded_data),\n",
    "    columns=encoded_data.columns\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 6ï¸âƒ£ Output finale e riepilogo\n",
    "# ================================\n",
    "print(f\"âœ… Dataset pronto per l'autoencoder: {X_autoencoder.shape}\")\n",
    "print(f\"ğŸ”¹ Numero di feature totali: {X_autoencoder.shape[1]}\")\n",
    "print(f\"ğŸ”¹ Classi multiclasse bilanciate: {y_multiclass.unique().tolist()}\")\n",
    "print(f\"ğŸ”¹ Classi binarie: {y_binary.unique().tolist()}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Distribuzione finale delle classi (bilanciato):\")\n",
    "display(y_multiclass.value_counts())\n",
    "\n",
    "print(\"âœ… Fine preparazione: X_autoencoder, y_multiclass, y_binary pronti per i modelli.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5: Addestramento Autoencoder con logging avanzato,\n",
    "# EarlyStopping, salvataggio best epoch e grafico\n",
    "# ==========================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # riduce warning TF/CUDA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Pulizia memoria Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"ğŸ—ï¸ Inizio costruzione e training dell'Autoencoder...\")\n",
    "\n",
    "# 1ï¸âƒ£ Parametri\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "latent_dim = 16\n",
    "\n",
    "# 2ï¸âƒ£ Costruzione autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu', name='latent_vector')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3ï¸âƒ£ EarlyStopping avanzato\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4ï¸âƒ£ Callback custom per log avanzato\n",
    "class ProgressLogger(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = 'â”' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "# 5ï¸âƒ£ Addestramento\n",
    "logger = ProgressLogger()\n",
    "history = autoencoder.fit(\n",
    "    X_autoencoder,\n",
    "    X_autoencoder,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop, logger]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Autoencoder addestrato con successo.\")\n",
    "\n",
    "# 6ï¸âƒ£ Recupero best epoch\n",
    "best_epoch_idx = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "best_train_loss = history.history['loss'][best_epoch_idx]\n",
    "best_val_loss = history.history['val_loss'][best_epoch_idx]\n",
    "best_time = logger.epoch_logs[best_epoch_idx]['time_s']\n",
    "\n",
    "print(f\"ğŸ† Best epoch: {best_epoch_idx+1}\")\n",
    "print(f\"    Train loss: {best_train_loss:.6f}\")\n",
    "print(f\"    Validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"    Tempo per epoca: {best_time:.2f}s\")\n",
    "\n",
    "# 7ï¸âƒ£ Estrazione encoder ottimale\n",
    "encoder = Model(inputs=input_layer, outputs=autoencoder.get_layer('latent_vector').output)\n",
    "encoder.save(\"encoder_best.keras\")\n",
    "\n",
    "# 8ï¸âƒ£ Estrazione embeddings latenti\n",
    "X_latent = encoder.predict(X_autoencoder)\n",
    "X_classifier = pd.DataFrame(X_latent, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "y_classifier = y_multiclass.reset_index(drop=True)\n",
    "\n",
    "# Controllo NaN\n",
    "assert not y_classifier.isna().any(), \"Errore: y_classifier contiene NaN\"\n",
    "\n",
    "print(f\"âœ… Embeddings generati: {X_classifier.shape}\")\n",
    "\n",
    "# 9ï¸âƒ£ Grafico Train vs Validation Loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.axvline(x=best_epoch_idx, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_idx+1})')\n",
    "plt.title(\"Andamento Train/Validation Loss per Epoca\", fontsize=14)\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f880bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6: Train/Test Split + StandardScaler + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"ğŸ—ï¸ Suddivisione dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1ï¸âƒ£ Split stratificato\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classifier,\n",
    "    y_classifier,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ… Split completato.\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 2ï¸âƒ£ Percentuali per categoria\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nğŸ“Š Distribuzione classi nel TRAIN:\")\n",
    "print(train_dist.round(2))\n",
    "print(\"\\nğŸ“Š Distribuzione classi nel TEST:\")\n",
    "print(test_dist.round(2))\n",
    "\n",
    "# 3ï¸âƒ£ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist.index, y=train_dist.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train\")\n",
    "sns.barplot(x=test_dist.index, y=test_dist.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4ï¸âƒ£ Scaling (solo train, poi test)\n",
    "print(\"\\nâš™ï¸ Applicazione StandardScaler sugli embeddings latenti...\")\n",
    "scaler_latent = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_latent.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_latent.transform(X_test), columns=X_test.columns)\n",
    "print(\"âœ… Scaling completato.\")\n",
    "\n",
    "# 5ï¸âƒ£ Visualizzazione confronto pre/post scaling su alcune feature\n",
    "sample_features = X_train.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features):\n",
    "    sns.histplot(X_train[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale\")\n",
    "    sns.histplot(X_train_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Dataset finale pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544472e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 7: Salvataggio dataset, scaler e encoder\n",
    "# ==========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"ğŸ’¾ Salvataggio dati e oggetti per training...\")\n",
    "\n",
    "# Crea cartella di output se non esiste\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "\n",
    "# 1ï¸âƒ£ Salvataggio dataset train/test\n",
    "X_train_scaled.to_csv(\"model_data/X_train_balanced.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"model_data/X_test_balanced.csv\", index=False)\n",
    "y_train.to_csv(\"model_data/y_train_balanced.csv\", index=False)\n",
    "y_test.to_csv(\"model_data/y_test_balanced.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Dataset salvati in 'model_data/'\")\n",
    "\n",
    "# 2ï¸âƒ£ Salvataggio StandardScaler\n",
    "joblib.dump(scaler_latent, \"model_data/scaler_latent.pkl\")\n",
    "print(\"âœ… Scaler salvato come 'scaler_latent.pkl'\")\n",
    "\n",
    "# 3ï¸âƒ£ (Opzionale) Salvataggio encoder\n",
    "encoder.save(\"model_data/encoder_best.keras\")\n",
    "print(\"âœ… Encoder salvato come 'encoder_best.keras'\")\n",
    "\n",
    "print(\"\\nğŸ¯ Tutti i dati pronti per l'addestramento dei modelli multiclasse!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d993959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
