{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25ae68",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Blocco 2: Cartella ZeekDataFall22\u001b[39;00m\n\u001b[32m     13\u001b[39m folder_datafall22 = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmaria\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mZeek_ML\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUWF-ZeekDataFall22\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfolder_data22\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m folder_datafall22.empty:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Uno dei dataset √® vuoto. Controlla i percorsi o i formati dei file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 1: Confronto Attributi Dataset Zeek\n",
    "# Controllo e uniformazione colonne tra UWF-ZeekData22 e UWF-ZeekDataFall22\n",
    "# ==========================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Blocchi 1 e 2: Definizione dei percorsi -------------\n",
    "# Blocco 1: Cartella ZeekData22\n",
    "folder_data22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekData22\"\n",
    "\n",
    "# Blocco 2: Cartella ZeekDataFall22\n",
    "folder_datafall22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "\n",
    "# Funzione per caricare tutti i parquet e csv di una cartella in un unico DataFrame\n",
    "def load_dataset(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    dfs = []\n",
    "    for f in all_files:\n",
    "        path = os.path.join(folder_path, f)\n",
    "        if f.endswith(\".parquet\"):\n",
    "            dfs.append(pd.read_parquet(path))\n",
    "        elif f.endswith(\".csv\"):\n",
    "            dfs.append(pd.read_csv(path))\n",
    "    if dfs:\n",
    "        # Concateno tutto in un unico dataframe\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------- Blocco 3: Caricamento dataset -------------\n",
    "df_22 = load_dataset(folder_data22)\n",
    "df_fall22 = load_dataset(folder_datafall22)\n",
    "\n",
    "# ------------- Blocco 4: Controllo colonne -------------\n",
    "cols_22 = set(df_22.columns)\n",
    "cols_fall22 = set(df_fall22.columns)\n",
    "\n",
    "# Colonne uniche per ciascun dataset\n",
    "unique_22 = cols_22 - cols_fall22\n",
    "unique_fall22 = cols_fall22 - cols_22\n",
    "common_cols = cols_22 & cols_fall22\n",
    "\n",
    "# ------------- Blocco 5: Stampa report differenze -------------\n",
    "print(\"=== BLOCCO 5: Report differenze tra dataset ===\\n\")\n",
    "print(f\"Totale colonne in ZeekData22: {len(cols_22)}\")\n",
    "print(f\"Totale colonne in ZeekDataFall22: {len(cols_fall22)}\\n\")\n",
    "\n",
    "print(\"Colonne presenti solo in ZeekData22:\")\n",
    "for col in sorted(unique_22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne presenti solo in ZeekDataFall22:\")\n",
    "for col in sorted(unique_fall22):\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nColonne comuni a entrambi i dataset:\")\n",
    "for col in sorted(common_cols):\n",
    "    print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2a‚Äì2b: Data Cleaning e Aggregazione (ZeekDataFall22)\n",
    "# ==========================================================\n",
    "# Scopo:\n",
    "# 1. Rimuovere duplicati\n",
    "# 2. Imputare valori mancanti\n",
    "# 3. Aggregare per sessione (uid) con feature significative\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella contenente i file parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# Caricamento dati in un unico DataFrame\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe prima del cleaning: {len(data)}\")\n",
    "print(f\"Totale colonne: {data.shape[1]}\")\n",
    "\n",
    "# ==========================\n",
    "# 1. Rimozione duplicati\n",
    "# ==========================\n",
    "duplicates_before = data.duplicated().sum()\n",
    "data = data.drop_duplicates()\n",
    "print(f\"Duplicati trovati e rimossi: {duplicates_before}\")\n",
    "\n",
    "# ==========================\n",
    "# 2. Imputazione valori mancanti\n",
    "# ==========================\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "imputation_count = 0\n",
    "for col in num_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "for col in cat_cols:\n",
    "    missing = data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        data[col].fillna(data[col].mode()[0] if not data[col].mode().empty else 'unknown', inplace=True)\n",
    "        imputation_count += missing\n",
    "\n",
    "print(f\"Totale valori imputati: {imputation_count}\")\n",
    "\n",
    "# ==========================\n",
    "# 3. Aggregazioni (session-level features)\n",
    "# ==========================\n",
    "print(\"üßπ Pulizia e aggregazione dati ZeekDataFall22...\\n\")\n",
    "\n",
    "if 'uid' not in data.columns:\n",
    "    print(\"‚ö†Ô∏è Colonna 'uid' mancante: creata automaticamente come indice numerico sequenziale.\")\n",
    "    data['uid'] = range(len(data))\n",
    "\n",
    "# Aggregazione per uid\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# Merge con il dataset originale\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "\n",
    "print(\"Prime righe del dataset dopo cleaning e aggregazioni:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2c: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Controllo colonna obbligatoria\n",
    "if 'label_technique' not in data.columns:\n",
    "    raise KeyError(\"‚ö†Ô∏è Manca la colonna 'label_technique' nel dataset caricato.\")\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"‚ö†Ô∏è Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Grafico prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione benigni -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"‚úÖ Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\nüìä Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"üìä Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento tactic -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\nüìä Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3a: Controllo valori nulli e riepilogo colonne/feature\n",
    "# ==========================================================\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"‚úÖ Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\nüìä Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\nüîπ Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\nüîπ Feature categoriali:\")\n",
    "print(cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Selezione feature numeriche\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "selected_features = variance[variance > 0.01].index.tolist()\n",
    "print(f\"‚úÖ Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "for col in selected_features:\n",
    "    Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    outlier_summary[col] = ((data[col]<lower) | (data[col]>upper)).sum()\n",
    "print(\"‚ö†Ô∏è Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Trasformazione outlier (Winsorization + log)\n",
    "# ================================\n",
    "print(\"üèóÔ∏è Applicazione trasformazioni per gestire outlier...\\n\")\n",
    "for col in selected_features:\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# üîß Fix: rimozione eventuali NaN residui\n",
    "data = data.dropna(subset=selected_features)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Analisi post-transform\n",
    "# ================================\n",
    "print(\"\\nüìÑ REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "print(\"üîπ Statistiche descrittive:\")\n",
    "display(data[selected_features].describe().T)\n",
    "\n",
    "print(\"\\nüìù Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere le distribuzioni.\\n\"\n",
    "    \"- Winsorization: limita i valori ai percentili 1% e 99%, riducendo l'impatto degli outlier.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria e migliora la stabilit√† numerica.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni pi√π compatte e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üìä Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"üìä Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"üìä Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"‚öñÔ∏è Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3f: Creazione dataset bilanciato (3 classi principali)\n",
    "# ==========================================================\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"‚öñÔ∏è Creazione dataset bilanciato tra le tre classi principali...\\n\")\n",
    "\n",
    "# Filtra solo le 3 classi principali\n",
    "balanced_data = data[data['label_tactic_reduced'].isin(['Resource Development', 'Reconnaissance', 'Discovery'])]\n",
    "\n",
    "# Trova la classe pi√π piccola\n",
    "min_count = balanced_data['label_tactic_reduced'].value_counts().min()\n",
    "\n",
    "# Esegui sottocampionamento per bilanciare\n",
    "balanced_samples = []\n",
    "for cls in ['Resource Development', 'Reconnaissance', 'Discovery']:\n",
    "    cls_df = balanced_data[balanced_data['label_tactic_reduced'] == cls]\n",
    "    cls_down = resample(cls_df, replace=False, n_samples=min_count, random_state=42)\n",
    "    balanced_samples.append(cls_down)\n",
    "\n",
    "balanced_data = pd.concat(balanced_samples).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verifica la nuova distribuzione\n",
    "print(\"üìä Distribuzione dopo bilanciamento:\")\n",
    "print(balanced_data['label_tactic_reduced'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=balanced_data['label_tactic_reduced'].value_counts().index,\n",
    "            y=balanced_data['label_tactic_reduced'].value_counts().values,\n",
    "            palette='pastel')\n",
    "plt.title(\"üìä Distribuzione Classi Bilanciate (3-class)\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4: Preparazione dataset per Autoencoder e classificazione\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"üèóÔ∏è Inizio preparazione dataset per autoencoder e classificazione (su dataset bilanciato)...\\n\")\n",
    "\n",
    "# ================================\n",
    "# 1Ô∏è‚É£ Definizione target e dataset di lavoro\n",
    "# ================================\n",
    "target_multiclass = 'label_tactic_reduced'\n",
    "\n",
    "# Controllo che le colonne esistano\n",
    "required_cols = ['label_binary', 'label_technique', 'label_tactic', target_multiclass]\n",
    "missing_cols = [c for c in required_cols if c not in balanced_data.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"‚ùå Colonne mancanti nel dataset bilanciato: {missing_cols}\")\n",
    "\n",
    "# Label (multiclasse e binaria)\n",
    "y_multiclass = balanced_data[target_multiclass].copy()\n",
    "y_binary = balanced_data['label_binary'].copy()\n",
    "\n",
    "# ================================\n",
    "# 2Ô∏è‚É£ Feature set\n",
    "# ================================\n",
    "feature_data = balanced_data.drop(columns=['label_binary', 'label_technique', 'label_tactic'])\n",
    "\n",
    "# ================================\n",
    "# 3Ô∏è‚É£ Conversione datetime ‚Üí numerico\n",
    "# ================================\n",
    "datetime_cols = feature_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data[col] = feature_data[col].astype('int64') / 1e9  # secondi Unix\n",
    "\n",
    "# ================================\n",
    "# 4Ô∏è‚É£ Frequency Encoding su categoriali\n",
    "# ================================\n",
    "cat_features = feature_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "encoded_data = feature_data.copy()\n",
    "\n",
    "for col in cat_features:\n",
    "    freq = encoded_data[col].value_counts(normalize=True)\n",
    "    encoded_data[col] = encoded_data[col].map(freq)\n",
    "\n",
    "# ================================\n",
    "# 5Ô∏è‚É£ Scaling MinMax per l‚ÄôAutoencoder\n",
    "# ================================\n",
    "scaler_auto = MinMaxScaler()\n",
    "X_autoencoder = pd.DataFrame(\n",
    "    scaler_auto.fit_transform(encoded_data),\n",
    "    columns=encoded_data.columns\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 6Ô∏è‚É£ Output finale e riepilogo\n",
    "# ================================\n",
    "print(f\"‚úÖ Dataset pronto per l'autoencoder: {X_autoencoder.shape}\")\n",
    "print(f\"üîπ Numero di feature totali: {X_autoencoder.shape[1]}\")\n",
    "print(f\"üîπ Classi multiclasse bilanciate: {y_multiclass.unique().tolist()}\")\n",
    "print(f\"üîπ Classi binarie: {y_binary.unique().tolist()}\")\n",
    "\n",
    "print(\"\\nüìä Distribuzione finale delle classi (bilanciato):\")\n",
    "display(y_multiclass.value_counts())\n",
    "\n",
    "print(\"‚úÖ Fine preparazione: X_autoencoder, y_multiclass, y_binary pronti per i modelli.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5: Addestramento Autoencoder con logging avanzato,\n",
    "# EarlyStopping, salvataggio best epoch e grafico\n",
    "# ==========================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # riduce warning TF/CUDA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Pulizia memoria Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"üèóÔ∏è Inizio costruzione e training dell'Autoencoder...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Parametri\n",
    "input_dim = X_autoencoder.shape[1]\n",
    "latent_dim = 16\n",
    "\n",
    "# 2Ô∏è‚É£ Costruzione autoencoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(latent_dim, activation='relu', name='latent_vector')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3Ô∏è‚É£ EarlyStopping avanzato\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Callback custom per log avanzato\n",
    "class ProgressLogger(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '‚îÅ' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "# 5Ô∏è‚É£ Addestramento\n",
    "logger = ProgressLogger()\n",
    "history = autoencoder.fit(\n",
    "    X_autoencoder,\n",
    "    X_autoencoder,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop, logger]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Autoencoder addestrato con successo.\")\n",
    "\n",
    "# 6Ô∏è‚É£ Recupero best epoch\n",
    "best_epoch_idx = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "best_train_loss = history.history['loss'][best_epoch_idx]\n",
    "best_val_loss = history.history['val_loss'][best_epoch_idx]\n",
    "best_time = logger.epoch_logs[best_epoch_idx]['time_s']\n",
    "\n",
    "print(f\"üèÜ Best epoch: {best_epoch_idx+1}\")\n",
    "print(f\"    Train loss: {best_train_loss:.6f}\")\n",
    "print(f\"    Validation loss: {best_val_loss:.6f}\")\n",
    "print(f\"    Tempo per epoca: {best_time:.2f}s\")\n",
    "\n",
    "# 7Ô∏è‚É£ Estrazione encoder ottimale\n",
    "encoder = Model(inputs=input_layer, outputs=autoencoder.get_layer('latent_vector').output)\n",
    "encoder.save(\"encoder_best.keras\")\n",
    "\n",
    "# 8Ô∏è‚É£ Estrazione embeddings latenti\n",
    "X_latent = encoder.predict(X_autoencoder)\n",
    "X_classifier = pd.DataFrame(X_latent, columns=[f'latent_{i}' for i in range(latent_dim)])\n",
    "y_classifier = y_multiclass.reset_index(drop=True)\n",
    "\n",
    "# Controllo NaN\n",
    "assert not y_classifier.isna().any(), \"Errore: y_classifier contiene NaN\"\n",
    "\n",
    "print(f\"‚úÖ Embeddings generati: {X_classifier.shape}\")\n",
    "\n",
    "# 9Ô∏è‚É£ Grafico Train vs Validation Loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.axvline(x=best_epoch_idx, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_idx+1})')\n",
    "plt.title(\"Andamento Train/Validation Loss per Epoca\", fontsize=14)\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f880bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6: Train/Test Split + StandardScaler + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üèóÔ∏è Suddivisione dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1Ô∏è‚É£ Split stratificato\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classifier,\n",
    "    y_classifier,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Split completato.\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Percentuali per categoria\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Distribuzione classi nel TRAIN:\")\n",
    "print(train_dist.round(2))\n",
    "print(\"\\nüìä Distribuzione classi nel TEST:\")\n",
    "print(test_dist.round(2))\n",
    "\n",
    "# 3Ô∏è‚É£ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist.index, y=train_dist.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train\")\n",
    "sns.barplot(x=test_dist.index, y=test_dist.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Scaling (solo train, poi test)\n",
    "print(\"\\n‚öôÔ∏è Applicazione StandardScaler sugli embeddings latenti...\")\n",
    "scaler_latent = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_latent.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler_latent.transform(X_test), columns=X_test.columns)\n",
    "print(\"‚úÖ Scaling completato.\")\n",
    "\n",
    "# 5Ô∏è‚É£ Visualizzazione confronto pre/post scaling su alcune feature\n",
    "sample_features = X_train.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features):\n",
    "    sns.histplot(X_train[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale\")\n",
    "    sns.histplot(X_train_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset finale pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544472e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 7: Salvataggio dataset, scaler e encoder\n",
    "# ==========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"üíæ Salvataggio dati e oggetti per training...\")\n",
    "\n",
    "# Crea cartella di output se non esiste\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "\n",
    "# 1Ô∏è‚É£ Salvataggio dataset train/test\n",
    "X_train_scaled.to_csv(\"model_data/X_train_balanced.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"model_data/X_test_balanced.csv\", index=False)\n",
    "y_train.to_csv(\"model_data/y_train_balanced.csv\", index=False)\n",
    "y_test.to_csv(\"model_data/y_test_balanced.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset salvati in 'model_data/'\")\n",
    "\n",
    "# 2Ô∏è‚É£ Salvataggio StandardScaler\n",
    "joblib.dump(scaler_latent, \"model_data/scaler_latent.pkl\")\n",
    "print(\"‚úÖ Scaler salvato come 'scaler_latent.pkl'\")\n",
    "\n",
    "# 3Ô∏è‚É£ (Opzionale) Salvataggio encoder\n",
    "encoder.save(\"model_data/encoder_best.keras\")\n",
    "print(\"‚úÖ Encoder salvato come 'encoder_best.keras'\")\n",
    "\n",
    "print(\"\\nüéØ Tutti i dati pronti per l'addestramento dei modelli multiclasse!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d993959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
