{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO: Data Cleaning e Aggregazione ottimizzato (ZeekDataFall22)\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Percorso cartella parquet\n",
    "folder_path = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekDataFall22\"\n",
    "parquet_files = glob.glob(os.path.join(folder_path, \"*.parquet\"))\n",
    "\n",
    "# -----------------------\n",
    "# 1️⃣ Caricamento incrementale\n",
    "# -----------------------\n",
    "dfs = []\n",
    "for file in tqdm(parquet_files, desc=\"Caricamento file parquet\"):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "data = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Totale righe iniziali: {len(data)}\")\n",
    "print(f\"Totale colonne iniziali: {data.shape[1]}\")\n",
    "\n",
    "# -----------------------\n",
    "# 2️⃣ Conversione object → category per risparmio RAM\n",
    "# -----------------------\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "# -----------------------\n",
    "# 3️⃣ Analisi valori mancanti\n",
    "# -----------------------\n",
    "col_summary = pd.DataFrame({\n",
    "    'dtype': data.dtypes,\n",
    "    'num_missing': data.isna().sum(),\n",
    "    'perc_missing': data.isna().mean() * 100\n",
    "}).sort_values('perc_missing', ascending=False)\n",
    "display(col_summary)\n",
    "\n",
    "# -----------------------\n",
    "# 4️⃣ Eliminazione colonne con troppi NaN (>50%)\n",
    "# -----------------------\n",
    "threshold = 50\n",
    "cols_to_drop = col_summary[col_summary['perc_missing'] > threshold].index.tolist()\n",
    "if cols_to_drop:\n",
    "    data.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Colonne eliminate per troppi NaN (> {threshold}%): {cols_to_drop}\")\n",
    "else:\n",
    "    print(\"Nessuna colonna da eliminare per troppi NaN.\")\n",
    "\n",
    "# -----------------------\n",
    "# 5️⃣ Rimozione duplicati (solo colonne chiave per risparmio RAM)\n",
    "# -----------------------\n",
    "subset_cols = ['uid', 'ts', 'orig_bytes', 'resp_bytes'] if 'uid' in data.columns else None\n",
    "duplicates_before = data.duplicated(subset=subset_cols).sum()\n",
    "data = data.drop_duplicates(subset=subset_cols)\n",
    "print(f\"Duplicati rimossi: {duplicates_before}\")\n",
    "\n",
    "# -----------------------\n",
    "# 6️⃣ Imputazione valori mancanti\n",
    "# -----------------------\n",
    "num_cols = data.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = data.select_dtypes(include=['category']).columns\n",
    "\n",
    "for col in num_cols:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "for col in cat_cols:\n",
    "    if data[col].isna().any():\n",
    "        mode_val = data[col].mode()\n",
    "        data[col].fillna(mode_val[0] if not mode_val.empty else 'unknown', inplace=True)\n",
    "\n",
    "# -----------------------\n",
    "# 7️⃣ Aggregazioni session-level features\n",
    "# -----------------------\n",
    "if 'uid' not in data.columns:\n",
    "    data['uid'] = range(len(data))\n",
    "\n",
    "session_features = data.groupby('uid').agg(\n",
    "    total_orig_bytes=('orig_bytes', 'sum'),\n",
    "    total_resp_bytes=('resp_bytes', 'sum'),\n",
    "    total_orig_pkts=('orig_pkts', 'sum'),\n",
    "    total_resp_pkts=('resp_pkts', 'sum'),\n",
    "    mean_duration=('duration', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "data = pd.merge(data, session_features, on='uid', how='left')\n",
    "print(f\"Totale sessioni aggregate: {session_features.shape[0]}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8️⃣ Anteprima finale\n",
    "# -----------------------\n",
    "display(data.head())\n",
    "print(\"✅ Data Cleaning e Aggregazioni completati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 2c: Rimozione traffico benigno (\"none\") per multiclasse\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Controllo colonna obbligatoria\n",
    "if 'label_technique' not in data.columns:\n",
    "    raise KeyError(\"⚠️ Manca la colonna 'label_technique' nel dataset caricato.\")\n",
    "\n",
    "# ----- Prima della rimozione -----\n",
    "total_count = len(data)\n",
    "none_count = (data['label_technique'] == 'none').sum()\n",
    "print(f\"⚠️ Campioni benigni rilevati: {none_count} / {total_count} ({none_count/total_count*100:.2f}%)\")\n",
    "\n",
    "# Grafico prima della rimozione\n",
    "attack_counts_before = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_before.index, x=attack_counts_before.values, palette='viridis')\n",
    "plt.title(\"📊 Distribuzione categorie di attacco PRIMA della rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Rimozione benigni -----\n",
    "data = data[data['label_technique'] != 'none'].reset_index(drop=True)\n",
    "print(f\"✅ Dopo rimozione benigni: {len(data)} righe rimanenti.\")\n",
    "\n",
    "# ----- Dopo la rimozione -----\n",
    "attack_counts_after = data['label_technique'].value_counts().sort_values(ascending=False)\n",
    "attack_percent_after = (attack_counts_after / len(data) * 100).round(2)\n",
    "attack_df_after = pd.DataFrame({\n",
    "    'Conteggio': attack_counts_after,\n",
    "    'Percentuale (%)': attack_percent_after\n",
    "})\n",
    "print(\"\\n📊 Distribuzione aggiornata per categorie di attacco (solo attacchi, benigni rimossi):\")\n",
    "display(attack_df_after)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=attack_counts_after.index, x=attack_counts_after.values, palette='magma')\n",
    "plt.title(\"📊 Distribuzione categorie di attacco DOPO la rimozione dei benigni\")\n",
    "plt.xlabel(\"Numero campioni\")\n",
    "plt.ylabel(\"Categoria di attacco\")\n",
    "plt.show()\n",
    "\n",
    "# ----- Aggiornamento tactic -----\n",
    "if 'tactic' in data.columns:\n",
    "    tactic_counts_after = data['tactic'].value_counts().sort_values(ascending=False)\n",
    "    tactic_percent_after = (tactic_counts_after / len(data) * 100).round(2)\n",
    "    tactic_df_after = pd.DataFrame({\n",
    "        'Conteggio': tactic_counts_after,\n",
    "        'Percentuale (%)': tactic_percent_after\n",
    "    })\n",
    "    print(\"\\n📊 Distribuzione aggiornata per tactic (benigni rimossi):\")\n",
    "    display(tactic_df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3a: Controllo valori nulli e riepilogo colonne/feature\n",
    "# ==========================================================\n",
    "\n",
    "# Controllo valori nulli residui\n",
    "null_counts = data.isna().sum()\n",
    "null_cols = null_counts[null_counts > 0]\n",
    "\n",
    "if len(null_cols) == 0:\n",
    "    print(\"✅ Non ci sono valori nulli residui.\")\n",
    "else:\n",
    "    print(\"⚠️ Colonne con valori nulli residui:\")\n",
    "    display(null_cols)\n",
    "\n",
    "# Riepilogo colonne e feature rimaste dopo data cleaning e aggregazioni\n",
    "print(\"\\n📊 Colonne e feature disponibili per l'analisi:\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Opzionale: possiamo separare feature numeriche e categoriali per la fase successiva\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"\\n🔹 Feature numeriche:\")\n",
    "print(num_features)\n",
    "print(\"\\n🔹 Feature categoriali:\")\n",
    "print(cat_features)\n",
    "# --------------------------\n",
    "# Salvataggio feature categoriali\n",
    "# --------------------------\n",
    "cat_features_df = data[cat_features].copy()\n",
    "cat_features_df.to_parquet(r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\categorical_features.parquet\", index=False)\n",
    "print(f\"💾 Feature categoriali salvate: {cat_features_df.shape[1]} colonne, {cat_features_df.shape[0]} righe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3b + 3c: Analisi feature numeriche, gestione outlier e trasformazione robusta\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ================================\n",
    "# Step 0: Pulizia sicura della label_binary\n",
    "# ================================\n",
    "data['label_binary_clean'] = data['label_binary'].map({True:1, False:0, 'True':1, 'False':0, 1:1, 0:0})\n",
    "data = data.dropna(subset=['label_binary_clean'])\n",
    "data['label_binary'] = data['label_binary_clean'].astype(int)\n",
    "data = data.drop(columns=['label_binary_clean'])\n",
    "\n",
    "# ================================\n",
    "# Step 1: Selezione feature numeriche\n",
    "# ================================\n",
    "num_features = data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "for col in ['label_binary','label_technique','label_tactic']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# ================================\n",
    "# Step 2: Varianza\n",
    "# ================================\n",
    "variance = data[num_features].var().sort_values(ascending=False)\n",
    "selected_features = variance[variance > 0.01].index.tolist()\n",
    "print(f\"✅ Feature con varianza significativa: {selected_features}\")\n",
    "\n",
    "# ================================\n",
    "# Step 3: Analisi outlier\n",
    "# ================================\n",
    "outlier_summary = {}\n",
    "for col in selected_features:\n",
    "    Q1, Q3 = data[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    outlier_summary[col] = ((data[col]<lower) | (data[col]>upper)).sum()\n",
    "print(\"⚠️ Numero di outlier trovati per feature:\")\n",
    "display(pd.Series(outlier_summary))\n",
    "\n",
    "# ================================\n",
    "# Step 4: Trasformazione outlier (Winsorization + log)\n",
    "# ================================\n",
    "print(\"🏗️ Applicazione trasformazioni per gestire outlier...\\n\")\n",
    "for col in selected_features:\n",
    "    lower = data[col].quantile(0.01)\n",
    "    upper = data[col].quantile(0.99)\n",
    "    data[col] = np.clip(data[col], lower, upper)\n",
    "    min_val = data[col].min()\n",
    "    offset = abs(min_val)+1e-6 if min_val <= 0 else 0\n",
    "    data[col] = np.log1p(data[col] + offset)\n",
    "    data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 🔧 Fix: rimozione eventuali NaN residui\n",
    "data = data.dropna(subset=selected_features)\n",
    "\n",
    "# ================================\n",
    "# Step 5: Analisi post-transform\n",
    "# ================================\n",
    "print(\"\\n📄 REPORT FINALE - Dataset post Winsorization + log\\n\")\n",
    "print(\"🔹 Statistiche descrittive:\")\n",
    "display(data[selected_features].describe().T)\n",
    "\n",
    "print(\"\\n📝 Motivazione trasformazione:\")\n",
    "print(\n",
    "    \"- Alcune feature avevano outlier estremi che potevano distorcere le distribuzioni.\\n\"\n",
    "    \"- Winsorization: limita i valori ai percentili 1% e 99%, riducendo l'impatto degli outlier.\\n\"\n",
    "    \"- Log-transform: riduce l'asimmetria e migliora la stabilità numerica.\\n\"\n",
    "    \"- Il dataset risultante ha distribuzioni più compatte e valori pronti per scaling/normalizzazione.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Step 6: Visualizzazione distribuzioni post-transform\n",
    "# ================================\n",
    "cols = 3\n",
    "rows = math.ceil(len(selected_features)/cols)\n",
    "plt.figure(figsize=(max(10, cols*5), max(5, rows*3)))\n",
    "for i, col in enumerate(selected_features, 1):\n",
    "    plt.subplot(rows, cols, i)\n",
    "    sns.histplot(data[col].dropna(), bins=50, kde=True, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3d: Analisi bilanciamento delle classi\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"📊 Analisi bilanciamento classi (binary e multiclass)...\\n\")\n",
    "\n",
    "# Binary\n",
    "binary_counts = data['label_binary'].value_counts()\n",
    "binary_perc = binary_counts / binary_counts.sum() * 100\n",
    "print(\"Distribuzione label_binary:\")\n",
    "print(pd.concat([binary_counts, binary_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Multiclass\n",
    "multiclass_counts = data['label_tactic'].value_counts()\n",
    "multiclass_perc = multiclass_counts / multiclass_counts.sum() * 100\n",
    "print(\"\\nDistribuzione label_tactic:\")\n",
    "print(pd.concat([multiclass_counts, multiclass_perc.round(2)], axis=1).rename(columns={0:'count',1:'%'}))\n",
    "\n",
    "# Grafici\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "binary_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Distribuzione Binary\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Conteggio\")\n",
    "\n",
    "multiclass_counts.plot(kind='bar', ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Distribuzione Multiclass (Tattiche)\")\n",
    "axes[1].set_xlabel(\"Tattica\")\n",
    "axes[1].set_ylabel(\"Conteggio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 3e: Consolidamento classi multiclass rare + class weights\n",
    "# ==========================================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Definizione classi principali\n",
    "main_classes = ['Resource Development', 'Reconnaissance', 'Discovery']\n",
    "\n",
    "# Creazione nuova colonna multiclass ridotta\n",
    "data['label_tactic_reduced'] = data['label_tactic'].apply(\n",
    "    lambda x: x if x in main_classes else 'Other'\n",
    ")\n",
    "\n",
    "# Distribuzione nuove classi\n",
    "reduced_counts = data['label_tactic_reduced'].value_counts()\n",
    "reduced_perc = (reduced_counts / reduced_counts.sum() * 100).round(2)\n",
    "reduced_df = pd.DataFrame({'Count': reduced_counts, 'Percent (%)': reduced_perc})\n",
    "print(\"📊 Distribuzione classi multiclass ridotte:\")\n",
    "display(reduced_df)\n",
    "\n",
    "# Grafico distribuzione\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=reduced_counts.index, y=reduced_counts.values, palette='pastel')\n",
    "plt.title(\"📊 Distribuzione classi multiclass ridotte\")\n",
    "plt.xlabel(\"Classe\")\n",
    "plt.ylabel(\"Conteggio\")\n",
    "plt.show()\n",
    "\n",
    "# ================================\n",
    "# Calcolo class weights (utile per training)\n",
    "# ================================\n",
    "classes = data['label_tactic_reduced'].unique()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(classes),\n",
    "    y=data['label_tactic_reduced']\n",
    ")\n",
    "class_weights_dict = dict(zip(classes, class_weights))\n",
    "print(\"⚖️ Class weights per le classi ridotte:\")\n",
    "for k,v in class_weights_dict.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "joblib.dump(class_weights_dict, \"model_data/class_weights_dict.pkl\")\n",
    "print(\"✅ Class weights salvati in 'model_data/class_weights_dict.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 4 Imbalanced: Preparazione dataset (classi sbilanciate con class weights)\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"🏗️ Inizio preparazione dataset per autoencoder e classificazione (dataset sbilanciato con class weights)...\\n\")\n",
    "\n",
    "# ================================\n",
    "# 1️⃣ Definizione target e dataset\n",
    "# ================================\n",
    "target_multiclass_imb = 'label_tactic_reduced'\n",
    "required_cols = ['label_binary', 'label_technique', 'label_tactic', target_multiclass_imb]\n",
    "missing_cols = [c for c in required_cols if c not in data.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f\"❌ Colonne mancanti nel dataset originale: {missing_cols}\")\n",
    "\n",
    "# Target (multiclass ridotto + binario)\n",
    "y_multiclass_imb = data[target_multiclass_imb].copy()\n",
    "y_binary_imb = data['label_binary'].copy()\n",
    "\n",
    "# ================================\n",
    "# 2️⃣ Feature set (senza label)\n",
    "# ================================\n",
    "feature_data_imb = data.drop(columns=['label_binary', 'label_technique', 'label_tactic', target_multiclass_imb])\n",
    "\n",
    "# ================================\n",
    "# 3️⃣ Conversione datetime → numerico (timestamp)\n",
    "# ================================\n",
    "datetime_cols = feature_data_imb.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    feature_data_imb[col] = feature_data_imb[col].astype('int64') / 1e9\n",
    "\n",
    "# ================================\n",
    "# 4️⃣ Selezione automatica feature numeriche ad alta varianza\n",
    "# ================================\n",
    "numerical_high_var_imb = [col for col in selected_features if col in feature_data_imb.columns]\n",
    "\n",
    "# ================================\n",
    "# 5️⃣ Frequency Encoding per feature categoriali\n",
    "# ================================\n",
    "cat_features_imb = feature_data_imb.select_dtypes(include=['object','category']).columns.tolist()\n",
    "encoded_data_imb = feature_data_imb.copy()\n",
    "\n",
    "for col in cat_features_imb:\n",
    "    freq = encoded_data_imb[col].value_counts(normalize=True)\n",
    "    encoded_data_imb[col] = encoded_data_imb[col].map(freq)\n",
    "\n",
    "# Mappatura finale feature (utile per pipeline o export)\n",
    "feature_mapping_imb = {col: col for col in numerical_high_var_imb + cat_features_imb}\n",
    "\n",
    "# ================================\n",
    "# 6️⃣ Scaling MinMax\n",
    "# ================================\n",
    "encoded_data_imb = encoded_data_imb[numerical_high_var_imb + cat_features_imb]\n",
    "\n",
    "scaler_auto_imb = MinMaxScaler()\n",
    "X_autoencoder_imb = pd.DataFrame(\n",
    "    scaler_auto_imb.fit_transform(encoded_data_imb),\n",
    "    columns=encoded_data_imb.columns\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 7️⃣ Salvataggio modelli e pesi\n",
    "# ================================\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "joblib.dump(scaler_auto_imb, \"model_data/scaler_auto_imbalanced.pkl\")\n",
    "joblib.dump(feature_mapping_imb, \"model_data/feature_mapping_imbalanced.pkl\")\n",
    "\n",
    "print(f\"✅ Dataset sbilanciato pronto: {X_autoencoder_imb.shape}\")\n",
    "print(f\"🔹 Numero di feature totali: {X_autoencoder_imb.shape[1]}\")\n",
    "print(f\"🔹 Classi multiclass (sbilanciate): {y_multiclass_imb.unique().tolist()}\")\n",
    "\n",
    "# Carica class weights salvati nel blocco precedente\n",
    "class_weights_dict = joblib.load(\"model_data/class_weights_dict.pkl\")\n",
    "print(f\"🔹 Class weights caricati da 'class_weights_dict.pkl'\")\n",
    "\n",
    "print(\"\\n📊 Distribuzione classi (originale, sbilanciata):\")\n",
    "display(y_multiclass_imb.value_counts())\n",
    "\n",
    "print(\"✅ Fine preparazione: X_autoencoder_imb, y_multiclass_imb, y_binary_imb pronti per modelli con class weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a50a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 5 IMBALANCED: Addestramento Autoencoder con logging avanzato\n",
    "# ==========================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # riduce warning TF/CUDA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Pulizia memoria Keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"🏗️ Inizio costruzione e training dell'Autoencoder - Sbilanciato..\")\n",
    "\n",
    "# 1️⃣ Parametri\n",
    "input_dim_imb = X_autoencoder_imb.shape[1]\n",
    "latent_dim_imb = 16\n",
    "\n",
    "# 2️⃣ Costruzione autoencoder\n",
    "input_layer_imb = Input(shape=(input_dim_imb,))\n",
    "encoded_imb = Dense(64, activation='relu')(input_layer_imb)\n",
    "encoded_imb = Dense(32, activation='relu')(encoded_imb)\n",
    "encoded_imb = Dense(latent_dim_imb, activation='relu', name='latent_vector_imb')(encoded_imb)\n",
    "decoded_imb = Dense(32, activation='relu')(encoded_imb)\n",
    "decoded_imb = Dense(64, activation='relu')(decoded_imb)\n",
    "decoded_imb = Dense(input_dim_imb, activation='sigmoid')(decoded_imb)\n",
    "\n",
    "autoencoder_imb = Model(inputs=input_layer_imb, outputs=decoded_imb)\n",
    "autoencoder_imb.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 3️⃣ EarlyStopping avanzato\n",
    "early_stop_imb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=1e-4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4️⃣ Callback custom con logging avanzato e tempo per epoca\n",
    "class ProgressLoggerImb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': logs['loss'],\n",
    "            'val_loss': logs['val_loss'],\n",
    "            'time_s': elapsed\n",
    "        })\n",
    "        bar_len = 30\n",
    "        progress = int(bar_len * (epoch+1)/self.params['epochs'])\n",
    "        bar = '━' * progress + '-' * (bar_len - progress)\n",
    "        print(f\"\\rEpoch {epoch+1}/{self.params['epochs']} [{bar}] \"\n",
    "              f\"loss: {logs['loss']:.6f} | val_loss: {logs['val_loss']:.6f} | tempo: {elapsed:.2f}s\", end='\\n')\n",
    "\n",
    "# 5️⃣ Training\n",
    "logger_imb = ProgressLoggerImb()\n",
    "history_imb = autoencoder_imb.fit(\n",
    "    X_autoencoder_imb,\n",
    "    X_autoencoder_imb,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stop_imb, logger_imb]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Autoencoder IMBALANCED addestrato con successo.\")\n",
    "\n",
    "# 6️⃣ Recupero best epoch\n",
    "best_epoch_idx_imb = history_imb.history['val_loss'].index(min(history_imb.history['val_loss']))\n",
    "best_train_loss_imb = history_imb.history['loss'][best_epoch_idx_imb]\n",
    "best_val_loss_imb = history_imb.history['val_loss'][best_epoch_idx_imb]\n",
    "best_time_imb = logger_imb.epoch_logs[best_epoch_idx_imb]['time_s']\n",
    "\n",
    "print(f\"🏆 Best epoch: {best_epoch_idx_imb+1}\")\n",
    "print(f\"    Train loss: {best_train_loss_imb:.6f}\")\n",
    "print(f\"    Validation loss: {best_val_loss_imb:.6f}\")\n",
    "print(f\"    Tempo per epoca: {best_time_imb:.2f}s\")\n",
    "\n",
    "# 7️⃣ Estrazione encoder ottimale e generazione embeddings\n",
    "encoder_imb = Model(inputs=input_layer_imb, outputs=autoencoder_imb.get_layer('latent_vector_imb').output)\n",
    "\n",
    "X_latent_imb = encoder_imb.predict(X_autoencoder_imb)\n",
    "X_classifier_imb = pd.DataFrame(X_latent_imb, columns=[f'latent_imb_{i}' for i in range(latent_dim_imb)])\n",
    "y_classifier_imb = y_multiclass_imb.reset_index(drop=True)\n",
    "\n",
    "# Controllo NaN\n",
    "assert not y_classifier_imb.isna().any(), \"Errore: y_classifier_imb contiene NaN\"\n",
    "\n",
    "print(f\"✅ Embeddings generati: {X_classifier_imb.shape}\")\n",
    "\n",
    "# 8️⃣ Grafico Train vs Validation Loss\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history_imb.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history_imb.history['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.axvline(x=best_epoch_idx_imb, color='r', linestyle='--', label=f'Best Epoch ({best_epoch_idx_imb+1})')\n",
    "plt.title(\"Autoencoder IMBALANCED - Andamento Train/Validation Loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 9️⃣ Salvataggio modelli e dataset latenti\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "autoencoder_imb.save(\"model_data/autoencoder_imbalanced.h5\")\n",
    "encoder_imb.save(\"model_data/encoder_imbalanced.h5\")\n",
    "joblib.dump(X_classifier_imb, \"model_data/X_classifier_imbalanced.pkl\")\n",
    "joblib.dump(y_classifier_imb, \"model_data/y_classifier_imbalanced.pkl\")\n",
    "\n",
    "print(\"💾 Modelli e dataset latenti salvati in 'model_data/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 6 (Imbalanced): Train/Test Split + Scaling + Analisi distribuzioni\n",
    "# ==========================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🏗️ Suddivisione IMBALANCED dataset in train/test e analisi bilanciamento classi...\")\n",
    "\n",
    "# 1️⃣ Split stratificato\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_classifier_imb,\n",
    "    y_classifier_imb,\n",
    "    test_size=0.2,\n",
    "    stratify=y_classifier_imb,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Split completato (Imbalanced).\")\n",
    "print(f\"Train: {X_train_imb.shape}, Test: {X_test_imb.shape}\")\n",
    "\n",
    "# 2️⃣ Percentuali per categoria\n",
    "train_dist_imb = y_train_imb.value_counts(normalize=True) * 100\n",
    "test_dist_imb = y_test_imb.value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\n📊 Distribuzione classi nel TRAIN (Imbalanced):\")\n",
    "print(train_dist_imb.round(2))\n",
    "print(\"\\n📊 Distribuzione classi nel TEST (Imbalanced):\")\n",
    "print(test_dist_imb.round(2))\n",
    "\n",
    "# 3️⃣ Grafico distribuzione classi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(x=train_dist_imb.index, y=train_dist_imb.values, ax=axes[0])\n",
    "axes[0].set_title(\"Distribuzione classi - Train (Imbalanced)\")\n",
    "sns.barplot(x=test_dist_imb.index, y=test_dist_imb.values, ax=axes[1])\n",
    "axes[1].set_title(\"Distribuzione classi - Test (Imbalanced)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4️⃣ Scaling\n",
    "print(\"\\n⚙️ Applicazione StandardScaler sugli embeddings latenti (Imbalanced)...\")\n",
    "scaler_latent_imb = StandardScaler()\n",
    "X_train_imb_scaled = pd.DataFrame(scaler_latent_imb.fit_transform(X_train_imb), columns=X_train_imb.columns)\n",
    "X_test_imb_scaled = pd.DataFrame(scaler_latent_imb.transform(X_test_imb), columns=X_test_imb.columns)\n",
    "print(\"✅ Scaling completato (Imbalanced).\")\n",
    "\n",
    "# 5️⃣ Visualizzazione confronto pre/post scaling\n",
    "sample_features_imb = X_train_imb.columns[:5]\n",
    "fig, axes = plt.subplots(len(sample_features_imb), 2, figsize=(10, 12))\n",
    "for i, feat in enumerate(sample_features_imb):\n",
    "    sns.histplot(X_train_imb[feat], ax=axes[i, 0], kde=True)\n",
    "    axes[i, 0].set_title(f\"{feat} - Originale (Imbalanced)\")\n",
    "    sns.histplot(X_train_imb_scaled[feat], ax=axes[i, 1], kde=True)\n",
    "    axes[i, 1].set_title(f\"{feat} - Scaled (Imbalanced)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Dataset IMBALANCED pronto per classificazione multiclasse.\")\n",
    "print(f\"Train: {X_train_imb_scaled.shape}, Test: {X_test_imb_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ae7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# BLOCCO 7 (Imbalanced): Salvataggio dataset, scaler e encoder\n",
    "# ==========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"💾 Salvataggio dati e modelli IMBALANCED...\")\n",
    "\n",
    "# 1️⃣ Creazione cartella di output\n",
    "os.makedirs(\"model_data_imbalanced\", exist_ok=True)\n",
    "\n",
    "# 2️⃣ Salvataggio dataset train/test\n",
    "X_train_imb_scaled.to_csv(\"model_data_imbalanced/X_train_imb.csv\", index=False)\n",
    "X_test_imb_scaled.to_csv(\"model_data_imbalanced/X_test_imb.csv\", index=False)\n",
    "y_train_imb.to_csv(\"model_data_imbalanced/y_train_imb.csv\", index=False)\n",
    "y_test_imb.to_csv(\"model_data_imbalanced/y_test_imb.csv\", index=False)\n",
    "print(\"✅ Dataset IMBALANCED salvati in 'model_data_imbalanced/'\")\n",
    "\n",
    "# 3️⃣ Salvataggio scaler\n",
    "joblib.dump(scaler_latent_imb, \"model_data_imbalanced/scaler_latent_imb.pkl\")\n",
    "print(\"✅ Scaler IMBALANCED salvato come 'scaler_latent_imb.pkl'\")\n",
    "\n",
    "# 4️⃣ Salvataggio encoder (dall’autoencoder imbalanced)\n",
    "encoder_imb.save(\"model_data_imbalanced/encoder_imb_best.keras\")\n",
    "print(\"✅ Encoder IMBALANCED salvato come 'encoder_imb_best.keras'\")\n",
    "\n",
    "print(\"\\n🎯 Tutti i dati e modelli IMBALANCED pronti per il training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
