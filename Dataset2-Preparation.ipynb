{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c24f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 14/14 [01:16<00:00,  5.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_data22 = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\UWF-ZeekData22\"\n",
    "output_folder = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_chunks\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "all_files = [f for f in os.listdir(folder_data22) if f.endswith(\".parquet\") or f.endswith(\".csv\")]\n",
    "\n",
    "for i, f in enumerate(tqdm(all_files, desc=\"Processing files\")):\n",
    "    file_path = os.path.join(folder_data22, f)\n",
    "    \n",
    "    # Caricamento file\n",
    "    if f.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    # =================\n",
    "    # Rimozione duplicati\n",
    "    # =================\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # =================\n",
    "    # Imputazione valori mancanti\n",
    "    # =================\n",
    "    num_cols = df.select_dtypes(include=['int64','float64']).columns\n",
    "    cat_cols = df.select_dtypes(include=['object','category']).columns\n",
    "    \n",
    "    for col in num_cols:\n",
    "        if df[col].isna().any():\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "    for col in cat_cols:\n",
    "        if df[col].isna().any():\n",
    "            df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'unknown', inplace=True)\n",
    "    \n",
    "    # =================\n",
    "    # Aggregazioni session-level se 'uid' presente\n",
    "    # =================\n",
    "    if 'uid' in df.columns:\n",
    "        session_features = df.groupby('uid').agg(\n",
    "            total_orig_bytes=('orig_bytes','sum'),\n",
    "            total_resp_bytes=('resp_bytes','sum'),\n",
    "            total_orig_pkts=('orig_pkts','sum'),\n",
    "            total_resp_pkts=('resp_pkts','sum'),\n",
    "            mean_duration=('duration','mean')\n",
    "        ).reset_index()\n",
    "        df = pd.merge(df, session_features, on='uid', how='left')\n",
    "    else:\n",
    "        df['uid'] = range(len(df))\n",
    "    \n",
    "    # =================\n",
    "    # Salvataggio file pulito in chunk\n",
    "    # =================\n",
    "    df.to_parquet(os.path.join(output_folder, f\"cleaned_{f.replace('.csv','.parquet')}\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be586a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating cleaned chunks: 100%|██████████| 14/14 [00:14<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Totale righe dataset concatenato: 6182348\n",
      "✅ Totale colonne: 42\n",
      "📊 Colonne con valori nulli residui:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Null %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-02-13 - 2022-02-20</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-06 - 2022-02-13</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-16 - 2022-01-23</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-09 - 2022-01-16</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 - 2022-01-09</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-26 - 2022-01-02</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-19 - 2021-12-26</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-12 - 2021-12-19</th>\n",
       "      <td>6182332</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest_port</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitre_attack_tactics</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_port</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_ip</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protocol</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest_ip</th>\n",
       "      <td>4851845</td>\n",
       "      <td>78.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proto</th>\n",
       "      <td>1330519</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_tactic</th>\n",
       "      <td>1330503</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_ip_zeek</th>\n",
       "      <td>1330519</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_port_zeek</th>\n",
       "      <td>1330519</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest_port_zeek</th>\n",
       "      <td>1330519</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dest_ip_zeek</th>\n",
       "      <td>1330519</td>\n",
       "      <td>21.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_resp</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missed_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conn_state</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_duration</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_resp_pkts</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_orig_pkts</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_resp_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_orig_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_pkts</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community_id</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp_bytes</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>history</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_orig</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resp_pkts</th>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Null Count  Null %\n",
       "2022-02-13 - 2022-02-20     6182332  100.00\n",
       "2022-02-06 - 2022-02-13     6182332  100.00\n",
       "2022-01-16 - 2022-01-23     6182332  100.00\n",
       "2022-01-09 - 2022-01-16     6182332  100.00\n",
       "2022-01-02 - 2022-01-09     6182332  100.00\n",
       "2021-12-26 - 2022-01-02     6182332  100.00\n",
       "2021-12-19 - 2021-12-26     6182332  100.00\n",
       "2021-12-12 - 2021-12-19     6182332  100.00\n",
       "dest_port                   4851845   78.48\n",
       "mitre_attack_tactics        4851845   78.48\n",
       "src_port                    4851845   78.48\n",
       "src_ip                      4851845   78.48\n",
       "protocol                    4851845   78.48\n",
       "dest_ip                     4851845   78.48\n",
       "proto                       1330519   21.52\n",
       "label_tactic                1330503   21.52\n",
       "src_ip_zeek                 1330519   21.52\n",
       "src_port_zeek               1330519   21.52\n",
       "dest_port_zeek              1330519   21.52\n",
       "dest_ip_zeek                1330519   21.52\n",
       "resp_ip_bytes                    16    0.00\n",
       "orig_ip_bytes                    16    0.00\n",
       "local_resp                       16    0.00\n",
       "missed_bytes                     16    0.00\n",
       "duration                         16    0.00\n",
       "conn_state                       16    0.00\n",
       "mean_duration                    16    0.00\n",
       "orig_bytes                       16    0.00\n",
       "total_resp_pkts                  16    0.00\n",
       "total_orig_pkts                  16    0.00\n",
       "total_resp_bytes                 16    0.00\n",
       "total_orig_bytes                 16    0.00\n",
       "orig_pkts                        16    0.00\n",
       "service                          16    0.00\n",
       "ts                               16    0.00\n",
       "community_id                     16    0.00\n",
       "resp_bytes                       16    0.00\n",
       "history                          16    0.00\n",
       "datetime                         16    0.00\n",
       "local_orig                       16    0.00\n",
       "resp_pkts                        16    0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Rimosso duplicati residui: 221319\n",
      "🔹 Esecuzione aggregazioni session-level finali...\n",
      "✅ Aggregazioni completate per 4851835 sessioni.\n",
      "✅ Pulizia finale completata. Dataset pronto per analisi o test dei modelli.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_10844\\3311713836.py:91: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  final_df[col] = pd.to_datetime(final_df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Dataset finale salvato con successo: C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_final.parquet\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Percorsi\n",
    "# -----------------------------\n",
    "cleaned_folder = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_chunks\"\n",
    "output_file = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_final.parquet\"\n",
    "\n",
    "# Trova tutti i file puliti\n",
    "cleaned_files = glob.glob(os.path.join(cleaned_folder, \"*.parquet\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Concatenazione memory-safe\n",
    "# -----------------------------\n",
    "dfs = []\n",
    "for f in tqdm(cleaned_files, desc=\"Concatenating cleaned chunks\"):\n",
    "    dfs.append(pd.read_parquet(f))\n",
    "    if len(dfs) == 5:  # concatenazione a blocchi di 5 file\n",
    "        temp = pd.concat(dfs, ignore_index=True)\n",
    "        dfs = [temp]\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"✅ Totale righe dataset concatenato: {len(final_df)}\")\n",
    "print(f\"✅ Totale colonne: {final_df.shape[1]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Analisi valori nulli\n",
    "# -----------------------------\n",
    "null_counts = final_df.isna().sum()\n",
    "null_perc = (null_counts / len(final_df) * 100).round(2)\n",
    "null_df = pd.DataFrame({'Null Count': null_counts, 'Null %': null_perc})\n",
    "null_df = null_df[null_df['Null Count']>0].sort_values('Null %', ascending=False)\n",
    "if not null_df.empty:\n",
    "    print(\"📊 Colonne con valori nulli residui:\")\n",
    "    display(null_df)\n",
    "else:\n",
    "    print(\"✅ Nessun valore null residuo.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rimozione duplicati residui\n",
    "# -----------------------------\n",
    "duplicates = final_df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    final_df = final_df.drop_duplicates()\n",
    "    print(f\"⚠️ Rimosso duplicati residui: {duplicates}\")\n",
    "else:\n",
    "    print(\"✅ Nessun duplicato residuo trovato.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Aggregazioni session-level finali\n",
    "# -----------------------------\n",
    "if 'uid' in final_df.columns:\n",
    "    print(\"🔹 Esecuzione aggregazioni session-level finali...\")\n",
    "    session_features = final_df.groupby('uid').agg(\n",
    "        total_orig_bytes=('orig_bytes','sum'),\n",
    "        total_resp_bytes=('resp_bytes','sum'),\n",
    "        total_orig_pkts=('orig_pkts','sum'),\n",
    "        total_resp_pkts=('resp_pkts','sum'),\n",
    "        mean_duration=('duration','mean')\n",
    "    ).reset_index()\n",
    "    final_df = pd.merge(final_df, session_features, on='uid', how='left')\n",
    "    print(f\"✅ Aggregazioni completate per {final_df['uid'].nunique()} sessioni.\")\n",
    "else:\n",
    "    print(\"⚠️ Colonna 'uid' mancante, impossibile fare aggregazioni session-level.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Imputazione residua valori nulli (opzionale)\n",
    "# -----------------------------\n",
    "num_cols = final_df.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = final_df.select_dtypes(include=['object','category']).columns\n",
    "\n",
    "for col in num_cols:\n",
    "    if final_df[col].isna().any():\n",
    "        final_df[col].fillna(final_df[col].mean(), inplace=True)\n",
    "for col in cat_cols:\n",
    "    if final_df[col].isna().any():\n",
    "        final_df[col].fillna(final_df[col].mode()[0] if not final_df[col].mode().empty else 'unknown', inplace=True)\n",
    "\n",
    "print(\"✅ Pulizia finale completata. Dataset pronto per analisi o test dei modelli.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Conversione sicura tipi datetime e object\n",
    "# -----------------------------\n",
    "for col in final_df.columns:\n",
    "    # Se il nome della colonna suggerisce un timestamp o datetime\n",
    "    if 'time' in col.lower() or 'date' in col.lower():\n",
    "        try:\n",
    "            final_df[col] = pd.to_datetime(final_df[col], errors='coerce')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Alcune colonne potrebbero rimanere di tipo misto (object con numeri o liste)\n",
    "# Convertiamo tutto ciò che non è numerico o datetime in stringa\n",
    "for col in final_df.columns:\n",
    "    if final_df[col].dtype == 'object':\n",
    "        try:\n",
    "            final_df[col] = final_df[col].astype(str)\n",
    "        except Exception:\n",
    "            final_df[col] = final_df[col].apply(lambda x: str(x) if not pd.isna(x) else \"\")\n",
    "\n",
    "# -----------------------------\n",
    "# Salvataggio dataset finale\n",
    "# -----------------------------\n",
    "try:\n",
    "    final_df.to_parquet(output_file, index=False)\n",
    "    print(f\"💾 Dataset finale salvato con successo: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore durante il salvataggio in parquet: {e}\")\n",
    "    print(\"👉 Provo a salvare in formato CSV come fallback...\")\n",
    "    csv_output = output_file.replace('.parquet', '.csv')\n",
    "    final_df.to_csv(csv_output, index=False)\n",
    "    print(f\"💾 Dataset salvato in formato CSV: {csv_output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5920d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset caricato: 5961029 righe, 47 colonne\n",
      "⚔️ Campioni malevoli: 5961029 su 5961029 totali (100.00%)\n",
      "\n",
      "📊 Distribuzione per tipo di attacco (label_tactic):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attack_Type</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>5922625</td>\n",
       "      <td>99.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>36248</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>2087</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credential Access</td>\n",
       "      <td>28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Privilege Escalation</td>\n",
       "      <td>14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Exfiltration</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lateral Movement</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Resource Development</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Defense Evasion</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Initial Access</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Persistence</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dt_first_record</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dt_last_record</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dt_week_end</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dt_week_start</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_record_count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Attack_Type    Count  Percentage\n",
       "0                   none  5922625       99.36\n",
       "1         Reconnaissance    36248        0.61\n",
       "2              Discovery     2087        0.04\n",
       "3      Credential Access       28        0.00\n",
       "4   Privilege Escalation       14        0.00\n",
       "5           Exfiltration        8        0.00\n",
       "6       Lateral Movement        4        0.00\n",
       "7   Resource Development        4        0.00\n",
       "8        Defense Evasion        2        0.00\n",
       "9         Initial Access        2        0.00\n",
       "10           Persistence        2        0.00\n",
       "11       dt_first_record        1        0.00\n",
       "12        dt_last_record        1        0.00\n",
       "13           dt_week_end        1        0.00\n",
       "14         dt_week_start        1        0.00\n",
       "15    total_record_count        1        0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Distribuzione per tattica MITRE (mitre_attack_tactics):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MITRE_Tactic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>5956972</td>\n",
       "      <td>99.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discovery</td>\n",
       "      <td>2086</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reconnaissance</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MITRE_Tactic    Count  Percentage\n",
       "0            none  5956972       99.93\n",
       "1       Discovery     2086        0.03\n",
       "2  Reconnaissance     1971        0.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Caricamento dataset finale\n",
    "# -----------------------------\n",
    "final_df = pd.read_parquet(r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_final.parquet\")\n",
    "print(f\"✅ Dataset caricato: {len(final_df)} righe, {final_df.shape[1]} colonne\")\n",
    "\n",
    "# -----------------------------\n",
    "# Rimozione traffico benigno\n",
    "# -----------------------------\n",
    "benign_mask = final_df['label_tactic'].str.contains('Benign|Normal', case=False, na=False) | \\\n",
    "              final_df['mitre_attack_tactics'].str.contains('Benign|Normal', case=False, na=False)\n",
    "\n",
    "df_attacks = final_df[~benign_mask].copy()\n",
    "print(f\"⚔️ Campioni malevoli: {len(df_attacks)} su {len(final_df)} totali ({len(df_attacks)/len(final_df)*100:.2f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Distribuzione per tipo di attacco (label_tactic)\n",
    "# -----------------------------\n",
    "label_counts = (\n",
    "    df_attacks['label_tactic']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('Attack_Type')\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "label_counts['Percentage'] = (label_counts['Count'] / len(df_attacks) * 100).round(2)\n",
    "\n",
    "print(\"\\n📊 Distribuzione per tipo di attacco (label_tactic):\")\n",
    "display(label_counts)\n",
    "\n",
    "# -----------------------------\n",
    "# Distribuzione per tattica MITRE (mitre_attack_tactics)\n",
    "# -----------------------------\n",
    "tactic_counts = (\n",
    "    df_attacks['mitre_attack_tactics']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('MITRE_Tactic')\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "tactic_counts['Percentage'] = (tactic_counts['Count'] / len(df_attacks) * 100).round(2)\n",
    "\n",
    "print(\"\\n🧠 Distribuzione per tattica MITRE (mitre_attack_tactics):\")\n",
    "display(tactic_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab135b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging cleaned chunks:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging cleaned chunks: 100%|██████████| 14/14 [00:10<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset concatenato: 6,182,348 righe, 42 colonne\n",
      "🧹 Rimossi 1,330,513 duplicati (21.52%)\n",
      "🧾 Eliminate 14 colonne con >60% valori null: ['protocol', 'dest_ip', 'dest_port', 'src_port', 'src_ip', 'mitre_attack_tactics', '2021-12-12 - 2021-12-19', '2021-12-19 - 2021-12-26', '2021-12-26 - 2022-01-02', '2022-01-02 - 2022-01-09', '2022-01-09 - 2022-01-16', '2022-01-16 - 2022-01-23', '2022-02-06 - 2022-02-13', '2022-02-13 - 2022-02-20']\n",
      "🧩 Valori mancanti imputati (median/unknown)\n",
      "📊 Aggregazioni session-level completate\n",
      "\n",
      "📊 Distribuzione finale per label_tactic:\n",
      "\n",
      "         Attack_Type   Count  Percentage\n",
      "                None 4817498       99.29\n",
      "      Reconnaissance   34277        0.71\n",
      "   Credential Access      28        0.00\n",
      "        Exfiltration       8        0.00\n",
      "Privilege Escalation       6        0.00\n",
      "    Lateral Movement       4        0.00\n",
      "Resource Development       4        0.00\n",
      "     Defense Evasion       2        0.00\n",
      "           Discovery       1        0.00\n",
      "      Initial Access       1        0.00\n",
      "         Persistence       1        0.00\n",
      "     Dt_First_Record       1        0.00\n",
      "      Dt_Last_Record       1        0.00\n",
      "         Dt_Week_End       1        0.00\n",
      "       Dt_Week_Start       1        0.00\n",
      "  Total_Record_Count       1        0.00\n",
      "\n",
      "🕒 Colonne convertite in datetime: ['datetime']\n",
      "\n",
      "💾 Dataset finale salvato in: C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_merged_final.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ======================\n",
    "# PATHS\n",
    "# ======================\n",
    "input_folder = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_chunks\"\n",
    "output_file = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_merged_final.parquet\"\n",
    "\n",
    "# ======================\n",
    "# CONCATENAZIONE CHUNKS\n",
    "# ======================\n",
    "all_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "dfs = []\n",
    "for f in tqdm(all_files, desc=\"Merging cleaned chunks\"):\n",
    "    chunk = pd.read_parquet(f)\n",
    "    dfs.append(chunk)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs  # libera memoria\n",
    "print(f\"✅ Dataset concatenato: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n",
    "\n",
    "# ======================\n",
    "# RIMOZIONE DUPLICATI\n",
    "# ======================\n",
    "before_dups = len(df)\n",
    "if \"uid\" in df.columns:\n",
    "    df.drop_duplicates(subset=\"uid\", inplace=True)\n",
    "else:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "after_dups = len(df)\n",
    "print(f\"🧹 Rimossi {before_dups - after_dups:,} duplicati ({round((before_dups - after_dups)/before_dups*100, 2)}%)\")\n",
    "\n",
    "# ======================\n",
    "# PULIZIA COLONNE CON TROPPI NULL\n",
    "# ======================\n",
    "null_threshold = 0.6  # 60% null → colonna eliminata\n",
    "null_ratios = df.isna().mean()\n",
    "cols_to_drop = null_ratios[null_ratios > null_threshold].index.tolist()\n",
    "\n",
    "if cols_to_drop:\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"🧾 Eliminate {len(cols_to_drop)} colonne con >{null_threshold*100:.0f}% valori null: {cols_to_drop}\")\n",
    "\n",
    "# ======================\n",
    "# IMPUTAZIONE VALORI MANCANTI\n",
    "# ======================\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [np.float64, np.int64]:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    elif df[col].dtype == 'object':\n",
    "        df[col].fillna('unknown', inplace=True)\n",
    "\n",
    "print(\"🧩 Valori mancanti imputati (median/unknown)\")\n",
    "\n",
    "# ======================\n",
    "# FUSIONE ETICHETTE TATTICHE\n",
    "# ======================\n",
    "if 'label_tactic' in df.columns and 'mitre_attack_tactic' in df.columns:\n",
    "    df['attack_tactic_temp'] = df['label_tactic'].fillna('none')\n",
    "    df.loc[df['attack_tactic_temp'].isin(['none', '', 'unknown']), 'attack_tactic_temp'] = df['mitre_attack_tactic']\n",
    "elif 'mitre_attack_tactic' in df.columns:\n",
    "    df['attack_tactic_temp'] = df['mitre_attack_tactic']\n",
    "elif 'label_tactic' in df.columns:\n",
    "    df['attack_tactic_temp'] = df['label_tactic']\n",
    "else:\n",
    "    raise ValueError(\"❌ Nessuna colonna tattica trovata nel dataset.\")\n",
    "\n",
    "df['attack_tactic_temp'] = (\n",
    "    df['attack_tactic_temp']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace({'nan': 'none', '': 'none', 'unknown': 'none'})\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "df.drop(columns=['label_tactic', 'mitre_attack_tactic'], errors='ignore', inplace=True)\n",
    "df.rename(columns={'attack_tactic_temp': 'label_tactic'}, inplace=True)\n",
    "\n",
    "# ======================\n",
    "# AGGREGAZIONI SESSION-LEVEL\n",
    "# ======================\n",
    "if 'uid' in df.columns:\n",
    "    session_features = df.groupby('uid').agg(\n",
    "        total_orig_bytes=('orig_bytes', 'sum'),\n",
    "        total_resp_bytes=('resp_bytes', 'sum'),\n",
    "        total_orig_pkts=('orig_pkts', 'sum'),\n",
    "        total_resp_pkts=('resp_pkts', 'sum'),\n",
    "        mean_duration=('duration', 'mean')\n",
    "    ).reset_index()\n",
    "    df = pd.merge(df, session_features, on='uid', how='left')\n",
    "    print(\"📊 Aggregazioni session-level completate\")\n",
    "\n",
    "# ======================\n",
    "# TABELLONE LABELS\n",
    "# ======================\n",
    "label_counts = df['label_tactic'].value_counts().reset_index()\n",
    "label_counts.columns = ['Attack_Type', 'Count']\n",
    "label_counts['Percentage'] = (label_counts['Count'] / len(df) * 100).round(2)\n",
    "print(\"\\n📊 Distribuzione finale per label_tactic:\\n\")\n",
    "print(label_counts.to_string(index=False))\n",
    "\n",
    "# ======================\n",
    "# CONVERSIONE DATETIME\n",
    "# ======================\n",
    "datetime_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
    "for col in datetime_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "    except Exception:\n",
    "        df[col] = df[col].astype(str)\n",
    "print(f\"\\n🕒 Colonne convertite in datetime: {datetime_cols}\")\n",
    "\n",
    "# ======================\n",
    "# TIPOLOGIE COMPATIBILI PARQUET\n",
    "# ======================\n",
    "df['uid'] = df['uid'].astype(str)\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# ======================\n",
    "# SALVATAGGIO FINALE\n",
    "# ======================\n",
    "df.to_parquet(output_file, index=False)\n",
    "print(f\"\\n💾 Dataset finale salvato in: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3aa99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Feature numeriche salvate (16 colonne): ['resp_pkts', 'orig_ip_bytes', 'missed_bytes', 'duration', 'orig_pkts', 'resp_ip_bytes', 'dest_port', 'orig_bytes', 'resp_bytes', 'src_port', 'ts', 'total_orig_bytes', 'total_resp_bytes', 'total_orig_pkts', 'total_resp_pkts', 'mean_duration']\n",
      "💾 Feature categoriali salvate (10 colonne): ['service', 'protocol', 'conn_state', 'dest_ip', 'community_id', 'datetime', 'history', 'uid', 'src_ip', 'mitre_attack_tactics']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 0️⃣ RECUPERO FEATURE NUMERICHE E CATEGORICAL\n",
    "# ==========================================\n",
    "\n",
    "# Cartella dove salvare i parquet\n",
    "os.makedirs(\"model_data\", exist_ok=True)\n",
    "\n",
    "# Caricamento di un chunk di riferimento per identificare le feature\n",
    "sample_file = [f for f in os.listdir(input_folder) if f.endswith(\".parquet\")][0]\n",
    "df_sample = pd.read_parquet(os.path.join(input_folder, sample_file))\n",
    "\n",
    "# --------------------------\n",
    "# Feature numeriche\n",
    "# --------------------------\n",
    "num_features = df_sample.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "# Rimuovo colonne target o non informative\n",
    "for col in ['label_binary','label_technique','label_tactic','label_tactic_reduced','uid']:\n",
    "    if col in num_features:\n",
    "        num_features.remove(col)\n",
    "\n",
    "# Salvataggio parquet numeriche\n",
    "pd.DataFrame(columns=num_features).to_parquet(\"model_data/numerical_features.parquet\", index=False)\n",
    "print(f\"💾 Feature numeriche salvate ({len(num_features)} colonne): {num_features}\")\n",
    "\n",
    "# --------------------------\n",
    "# Feature categoriali\n",
    "# --------------------------\n",
    "cat_features = df_sample.select_dtypes(include=['object','category']).columns.tolist()\n",
    "# Salvataggio parquet categoriali\n",
    "pd.DataFrame(columns=cat_features).to_parquet(\"model_data/categorical_features.parquet\", index=False)\n",
    "print(f\"💾 Feature categoriali salvate ({len(cat_features)} colonne): {cat_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe582fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_data/scaler_auto.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m num_features = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mmodel_data/numerical_features.parquet\u001b[39m\u001b[33m\"\u001b[39m).columns.tolist()\n\u001b[32m     16\u001b[39m cat_features = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mmodel_data/categorical_features.parquet\u001b[39m\u001b[33m\"\u001b[39m).columns.tolist()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m scaler_auto = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_data/scaler_auto.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m freq_dict = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mmodel_data/freq_encoding_dict.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m encoder = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mmodel_data/autoencoder_encoder_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maria\\anaconda3\\envs\\zeek-ml\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_data/scaler_auto.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ==============================\n",
    "# 0️⃣ Caricamento dataset ZeekData22 già pulito\n",
    "# ==============================\n",
    "zeek22_file = r\"C:\\Users\\maria\\Desktop\\Zeek_ML\\processed_zeekdata22\\ZeekData22_final.parquet\"\n",
    "df_22 = pd.read_parquet(zeek22_file)\n",
    "print(f\"✅ ZeekData22 caricato: {df_22.shape[0]} righe, {df_22.shape[1]} colonne\")\n",
    "\n",
    "# ==============================\n",
    "# 1️⃣ Caricamento feature list usate in ZeekDataFall22\n",
    "# ==============================\n",
    "num_features_fall = pd.read_parquet(\"model_data/numerical_features.parquet\").columns.tolist()\n",
    "cat_features_fall = pd.read_parquet(\"model_data/categorical_features.parquet\").columns.tolist()\n",
    "\n",
    "print(f\"🔹 Numeriche Fall22: {len(num_features_fall)}\")\n",
    "print(f\"🔹 Categoriali Fall22: {len(cat_features_fall)}\")\n",
    "\n",
    "# ==============================\n",
    "# 2️⃣ Allineamento colonne numeriche\n",
    "# ==============================\n",
    "# Manteniamo solo le colonne numeriche comuni\n",
    "num_common = [col for col in num_features_fall if col in df_22.columns]\n",
    "# Se mancano colonne numeriche richieste → imputiamo a 0\n",
    "num_missing = [col for col in num_features_fall if col not in df_22.columns]\n",
    "for col in num_missing:\n",
    "    df_22[col] = 0.0\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ Allineamento colonne categoriali\n",
    "# ==============================\n",
    "cat_common = [col for col in cat_features_fall if col in df_22.columns]\n",
    "cat_missing = [col for col in cat_features_fall if col not in df_22.columns]\n",
    "for col in cat_missing:\n",
    "    df_22[col] = \"unknown\"\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ Frequency encoding su feature categoriali (stesso approccio Fall22)\n",
    "# ==============================\n",
    "for col in cat_features_fall:\n",
    "    freq = df_22[col].value_counts(normalize=True)\n",
    "    df_22[col] = df_22[col].map(freq).fillna(0.0)\n",
    "\n",
    "# ==============================\n",
    "# 5️⃣ Creazione dataset finale allineato\n",
    "# ==============================\n",
    "# Ordine esatto: numeriche ad alta varianza + categoriali encoded\n",
    "feature_order = num_features_fall + cat_features_fall\n",
    "X_22_aligned = df_22[feature_order].copy()\n",
    "\n",
    "# ==============================\n",
    "# 6️⃣ MinMax scaling (usando scaler dell'encoder Fall22 se salvato)\n",
    "# ==============================\n",
    "scaler_file = \"model_data/scaler_autoencoder.pkl\"  # se hai salvato lo scaler originale\n",
    "try:\n",
    "    scaler_auto = joblib.load(scaler_file)\n",
    "    X_22_scaled = pd.DataFrame(\n",
    "        scaler_auto.transform(X_22_aligned),\n",
    "        columns=X_22_aligned.columns\n",
    "    )\n",
    "    print(\"✅ Scaling MinMax applicato con scaler Fall22 salvato\")\n",
    "except Exception:\n",
    "    # fallback: scala tra 0 e 1 sul dataset Zeek22\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler_auto = MinMaxScaler()\n",
    "    X_22_scaled = pd.DataFrame(\n",
    "        scaler_auto.fit_transform(X_22_aligned),\n",
    "        columns=X_22_aligned.columns\n",
    "    )\n",
    "    print(\"⚠️ Fallback: scaling MinMax calcolato su ZeekData22\")\n",
    "\n",
    "# ==============================\n",
    "# 7️⃣ Caricamento encoder Fall22 e generazione embeddings latenti\n",
    "# ==============================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "encoder_file = \"model_data/encoder_best.keras\"\n",
    "encoder = load_model(encoder_file, compile=False)\n",
    "\n",
    "X_22_latent = encoder.predict(X_22_scaled)\n",
    "latent_dim = X_22_latent.shape[1]\n",
    "X_22_latent_df = pd.DataFrame(X_22_latent, columns=[f\"latent_{i}\" for i in range(latent_dim)])\n",
    "print(f\"✅ ZeekData22 trasformato in latent embeddings: {X_22_latent_df.shape}\")\n",
    "\n",
    "# ==============================\n",
    "# 8️⃣ Salvataggio dataset pronto come test set\n",
    "# ==============================\n",
    "X_22_latent_df.to_parquet(\"model_data/ZeekData22_test_embeddings.parquet\", index=False)\n",
    "print(\"💾 ZeekData22 pronto come test set per autoencoder Fall22 salvato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b391005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeek-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
